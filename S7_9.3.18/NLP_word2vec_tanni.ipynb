{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Requirement already satisfied: tqdm in c:\\users\\tangila\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tangila\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: gensim in c:\\users\\tangila\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from gensim)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from gensim)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from gensim)\n",
      "Requirement already satisfied: smart_open>=1.2.1 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from gensim)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from smart_open>=1.2.1->gensim)\n",
      "Requirement already satisfied: bz2file in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from smart_open>=1.2.1->gensim)\n",
      "Requirement already satisfied: requests in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from smart_open>=1.2.1->gensim)\n",
      "Requirement already satisfied: boto3 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from smart_open>=1.2.1->gensim)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from requests->smart_open>=1.2.1->gensim)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from requests->smart_open>=1.2.1->gensim)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from requests->smart_open>=1.2.1->gensim)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from requests->smart_open>=1.2.1->gensim)\n",
      "Requirement already satisfied: botocore<1.9.0,>=1.8.46 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from boto3->smart_open>=1.2.1->gensim)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from boto3->smart_open>=1.2.1->gensim)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from boto3->smart_open>=1.2.1->gensim)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from botocore<1.9.0,>=1.8.46->boto3->smart_open>=1.2.1->gensim)\n",
      "Requirement already satisfied: docutils>=0.10 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from botocore<1.9.0,>=1.8.46->boto3->smart_open>=1.2.1->gensim)\n",
      "Requirement already satisfied: spacy in c:\\users\\tangila\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already satisfied: murmurhash<0.27,>=0.26 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already satisfied: thinc<6.6.0,>=6.5.0 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already satisfied: pip<10.0.0,>=9.0.0 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already satisfied: six in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already satisfied: pathlib in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already satisfied: ujson>=1.35 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already satisfied: regex<2017.12.1,>=2017.4.1 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already satisfied: ftfy<5.0.0,>=4.4.2 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already satisfied: wrapt in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from thinc<6.6.0,>=6.5.0->spacy)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from thinc<6.6.0,>=6.5.0->spacy)\n",
      "Requirement already satisfied: cytoolz<0.9,>=0.8 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from thinc<6.6.0,>=6.5.0->spacy)\n",
      "Requirement already satisfied: termcolor in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from thinc<6.6.0,>=6.5.0->spacy)\n",
      "Requirement already satisfied: pyreadline>=1.7.1 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from dill<0.3,>=0.2->spacy)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already satisfied: html5lib in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from ftfy<5.0.0,>=4.4.2->spacy)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from ftfy<5.0.0,>=4.4.2->spacy)\n",
      "Requirement already satisfied: toolz>=0.8.0 in c:\\users\\tangila\\anaconda3\\lib\\site-packages (from cytoolz<0.9,>=0.8->thinc<6.6.0,>=6.5.0->spacy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'python3' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\tangila\\anaconda3\\lib\\site-packages\n"
     ]
    }
   ],
   "source": [
    "# Run this block to install dependencies [Remember to make the statement true]\n",
    "if 0 != 1:\n",
    "    !pip install pandas\n",
    "    !pip install tqdm\n",
    "    !pip install scikit-learn\n",
    "    !pip install gensim\n",
    "    !pip install spacy\n",
    "    !python3 -m spacy download en\n",
    "    !pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "DATA_LIMIT = 1000\n",
    "\n",
    "df = pd.read_csv('./imdb_master.csv', encoding='latin1')\n",
    "df_neg = df[df['label'] == 'neg']\n",
    "df_pos = df[df['label'] == 'pos']\n",
    "df = pd.concat((df_pos[:DATA_LIMIT], df_neg[:DATA_LIMIT]))\n",
    "\n",
    "def process_text(input_string, return_string=False, stem=False):\n",
    "    text = nlp(u'' + input_string)\n",
    "    if stem == True:\n",
    "        text = [tok.lemma_ for tok in text if (tok.is_alpha and not tok.is_stop)]\n",
    "    else:\n",
    "        text = [tok.lower_ for tok in text if (tok.is_alpha and not tok.is_stop)]\n",
    "    if return_string == True:\n",
    "        return \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2000/2000 [01:00<00:00, 33.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# Make this statement true to run from scratch [It takes time to process the text]\n",
    "if 1 == 1:\n",
    "    wordlist = []\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        wordlist.append(process_text(df['review'].iloc[i]))\n",
    "        \n",
    "    with open('vocabulary.txt', 'wb') as vocabulary:\n",
    "        pickle.dump(wordlist, vocabulary)\n",
    "    vocabulary.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "word2v_model = Word2Vec(window = 5, workers = multiprocessing.cpu_count(), iter = 100, min_count =1, hs=1, negative=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=0, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(word2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=22171, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "word2v_model.build_vocab(wordlist)\n",
    "print(word2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 29.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18998996, 19737700)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time word2v_model.train(wordlist, total_examples = word2v_model.corpus_count, epochs = word2v_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2v_model.wv['cat'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = Dictionary(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22171"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('suitably', 0.4540744423866272),\n",
       " ('trauma', 0.3898535966873169),\n",
       " ('fogging', 0.37884706258773804),\n",
       " ('untangled', 0.3671215772628784),\n",
       " ('ambassadors', 0.36553695797920227),\n",
       " ('affecting', 0.36449331045150757),\n",
       " ('tearjerker', 0.3600119352340698),\n",
       " ('slug', 0.35770124197006226),\n",
       " ('grasped', 0.35429099202156067),\n",
       " ('kali', 0.34955230355262756)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2v_model.wv.most_similar('bellucci')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 0.439160019159317),\n",
       " ('movies', 0.41975343227386475),\n",
       " ('fact', 0.3750477433204651),\n",
       " ('enjoy', 0.36571305990219116),\n",
       " ('remembered', 0.3597602844238281),\n",
       " ('realistic', 0.35765761137008667),\n",
       " ('ese', 0.3412356674671173),\n",
       " ('wow', 0.33522793650627136),\n",
       " ('cages', 0.32429051399230957),\n",
       " ('hinted', 0.3231661319732666)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2v_model.wv.most_similar(positive=['movie', 'good'], negative=['bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Create a numpy empty/random matrix with dimension of [Voc size+1 X, Embedding dimension]\\n2. Load the embeddings into that word\\n3. Create keras embedding layer with the same configuration and load weights there\\n4. Train a RNN/CNN to classify\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Create a numpy empty/random matrix with dimension of [Voc size+1 X, Embedding dimension]\n",
    "2. Load the embeddings into that word\n",
    "3. Create keras embedding layer with the same configuration and load weights there\n",
    "4. Train a RNN/CNN to classify\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding = np.zeros((len(vocabulary)+1, embedding_dim))\n",
    "#embedding[0] = 0\n",
    "#embedding[1] = word2v_model.wv[ vocabulary[0] ]\n",
    "\n",
    "\n",
    "for i in range(len(vocabulary)):\n",
    "        embedding[i+1] = word2v_model.wv[ vocabulary[i] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GRU, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Embedding??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd = Embedding(len(vocabulary)+1, output_dim = embedding_dim, weights =[embedding], trainable = False )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(embd.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "for review in wordlist:\n",
    "    X.append(np.array(vocabulary.doc2idx(review)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = pad_sequences(X, value = 0, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = np.concatenate((np.ones(1000), np.zeros(1000)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train....\n",
      "Train on 1400 samples, validate on 600 samples\n",
      "Epoch 1/15\n",
      "1400/1400 [==============================] - ETA: 1:19 - loss: 0.7287 - acc: 0.460 - ETA: 40s - loss: 0.7197 - acc: 0.488 - ETA: 26s - loss: 0.7060 - acc: 0.51 - ETA: 18s - loss: 0.6904 - acc: 0.54 - ETA: 13s - loss: 0.6852 - acc: 0.56 - ETA: 10s - loss: 0.6706 - acc: 0.59 - ETA: 7s - loss: 0.6673 - acc: 0.5949 - ETA: 5s - loss: 0.6641 - acc: 0.603 - ETA: 3s - loss: 0.6570 - acc: 0.614 - ETA: 1s - loss: 0.6481 - acc: 0.625 - 18s 13ms/step - loss: 0.6404 - acc: 0.6350 - val_loss: 0.9822 - val_acc: 0.2367\n",
      "Epoch 2/15\n",
      "1400/1400 [==============================] - ETA: 7s - loss: 0.5446 - acc: 0.718 - ETA: 6s - loss: 0.5484 - acc: 0.714 - ETA: 6s - loss: 0.5333 - acc: 0.734 - ETA: 5s - loss: 0.5432 - acc: 0.726 - ETA: 4s - loss: 0.5584 - acc: 0.710 - ETA: 3s - loss: 0.5464 - acc: 0.722 - ETA: 3s - loss: 0.5450 - acc: 0.725 - ETA: 2s - loss: 0.5408 - acc: 0.735 - ETA: 1s - loss: 0.5357 - acc: 0.740 - ETA: 0s - loss: 0.5333 - acc: 0.742 - 10s 7ms/step - loss: 0.5280 - acc: 0.7450 - val_loss: 1.0930 - val_acc: 0.2800\n",
      "Epoch 3/15\n",
      "1400/1400 [==============================] - ETA: 8s - loss: 0.4645 - acc: 0.773 - ETA: 7s - loss: 0.5117 - acc: 0.734 - ETA: 6s - loss: 0.4929 - acc: 0.768 - ETA: 5s - loss: 0.4856 - acc: 0.767 - ETA: 4s - loss: 0.4807 - acc: 0.778 - ETA: 3s - loss: 0.4818 - acc: 0.779 - ETA: 3s - loss: 0.4830 - acc: 0.779 - ETA: 2s - loss: 0.4794 - acc: 0.779 - ETA: 1s - loss: 0.4696 - acc: 0.783 - ETA: 0s - loss: 0.4675 - acc: 0.782 - 10s 7ms/step - loss: 0.4644 - acc: 0.7864 - val_loss: 1.0497 - val_acc: 0.3900\n",
      "Epoch 4/15\n",
      "1400/1400 [==============================] - ETA: 7s - loss: 0.4102 - acc: 0.796 - ETA: 6s - loss: 0.4246 - acc: 0.793 - ETA: 6s - loss: 0.4089 - acc: 0.809 - ETA: 5s - loss: 0.4078 - acc: 0.818 - ETA: 4s - loss: 0.4141 - acc: 0.820 - ETA: 3s - loss: 0.4148 - acc: 0.819 - ETA: 3s - loss: 0.3958 - acc: 0.825 - ETA: 2s - loss: 0.3968 - acc: 0.825 - ETA: 1s - loss: 0.3960 - acc: 0.823 - ETA: 0s - loss: 0.4009 - acc: 0.820 - 9s 7ms/step - loss: 0.3936 - acc: 0.8257 - val_loss: 0.9931 - val_acc: 0.5667\n",
      "Epoch 5/15\n",
      "1400/1400 [==============================] - ETA: 8s - loss: 0.3873 - acc: 0.828 - ETA: 6s - loss: 0.3331 - acc: 0.839 - ETA: 5s - loss: 0.3321 - acc: 0.846 - ETA: 5s - loss: 0.3072 - acc: 0.865 - ETA: 4s - loss: 0.3217 - acc: 0.862 - ETA: 3s - loss: 0.3119 - acc: 0.867 - ETA: 2s - loss: 0.3213 - acc: 0.859 - ETA: 2s - loss: 0.3149 - acc: 0.865 - ETA: 1s - loss: 0.3166 - acc: 0.861 - ETA: 0s - loss: 0.3118 - acc: 0.868 - 9s 6ms/step - loss: 0.3169 - acc: 0.8664 - val_loss: 0.5842 - val_acc: 0.7500\n",
      "Epoch 6/15\n",
      "1400/1400 [==============================] - ETA: 6s - loss: 0.2796 - acc: 0.898 - ETA: 5s - loss: 0.2948 - acc: 0.878 - ETA: 5s - loss: 0.2973 - acc: 0.877 - ETA: 4s - loss: 0.2925 - acc: 0.880 - ETA: 4s - loss: 0.2937 - acc: 0.881 - ETA: 3s - loss: 0.2919 - acc: 0.880 - ETA: 2s - loss: 0.2966 - acc: 0.877 - ETA: 1s - loss: 0.2992 - acc: 0.874 - ETA: 1s - loss: 0.3000 - acc: 0.875 - ETA: 0s - loss: 0.2997 - acc: 0.876 - 8s 6ms/step - loss: 0.2942 - acc: 0.8814 - val_loss: 0.7470 - val_acc: 0.6550\n",
      "Epoch 7/15\n",
      "1400/1400 [==============================] - ETA: 6s - loss: 0.2330 - acc: 0.929 - ETA: 5s - loss: 0.2883 - acc: 0.894 - ETA: 5s - loss: 0.3019 - acc: 0.880 - ETA: 4s - loss: 0.3019 - acc: 0.873 - ETA: 3s - loss: 0.3037 - acc: 0.878 - ETA: 3s - loss: 0.2925 - acc: 0.884 - ETA: 2s - loss: 0.2908 - acc: 0.885 - ETA: 1s - loss: 0.2915 - acc: 0.882 - ETA: 1s - loss: 0.2908 - acc: 0.883 - ETA: 0s - loss: 0.2842 - acc: 0.887 - 8s 6ms/step - loss: 0.2762 - acc: 0.8929 - val_loss: 0.7128 - val_acc: 0.6850\n",
      "Epoch 8/15\n",
      "1400/1400 [==============================] - ETA: 6s - loss: 0.2503 - acc: 0.906 - ETA: 5s - loss: 0.2591 - acc: 0.890 - ETA: 5s - loss: 0.2384 - acc: 0.898 - ETA: 4s - loss: 0.2270 - acc: 0.906 - ETA: 3s - loss: 0.2329 - acc: 0.904 - ETA: 3s - loss: 0.2304 - acc: 0.906 - ETA: 2s - loss: 0.2470 - acc: 0.901 - ETA: 1s - loss: 0.2501 - acc: 0.901 - ETA: 1s - loss: 0.2584 - acc: 0.898 - ETA: 0s - loss: 0.2556 - acc: 0.898 - 8s 6ms/step - loss: 0.2483 - acc: 0.9029 - val_loss: 0.7203 - val_acc: 0.7000\n",
      "Epoch 9/15\n",
      "1400/1400 [==============================] - ETA: 6s - loss: 0.2684 - acc: 0.882 - ETA: 6s - loss: 0.2283 - acc: 0.902 - ETA: 5s - loss: 0.2242 - acc: 0.908 - ETA: 4s - loss: 0.2185 - acc: 0.916 - ETA: 4s - loss: 0.2111 - acc: 0.921 - ETA: 3s - loss: 0.2114 - acc: 0.919 - ETA: 2s - loss: 0.2236 - acc: 0.917 - ETA: 2s - loss: 0.2271 - acc: 0.916 - ETA: 1s - loss: 0.2254 - acc: 0.917 - ETA: 0s - loss: 0.2302 - acc: 0.912 - 8s 6ms/step - loss: 0.2232 - acc: 0.9186 - val_loss: 0.5952 - val_acc: 0.7383\n",
      "Epoch 10/15\n",
      "1400/1400 [==============================] - ETA: 6s - loss: 0.1648 - acc: 0.937 - ETA: 5s - loss: 0.1763 - acc: 0.933 - ETA: 5s - loss: 0.2227 - acc: 0.914 - ETA: 4s - loss: 0.2492 - acc: 0.902 - ETA: 3s - loss: 0.2458 - acc: 0.903 - ETA: 3s - loss: 0.2416 - acc: 0.904 - ETA: 2s - loss: 0.2310 - acc: 0.911 - ETA: 1s - loss: 0.2216 - acc: 0.916 - ETA: 1s - loss: 0.2217 - acc: 0.915 - ETA: 0s - loss: 0.2204 - acc: 0.914 - 8s 6ms/step - loss: 0.2174 - acc: 0.9150 - val_loss: 0.8128 - val_acc: 0.6883\n",
      "Epoch 11/15\n",
      "1400/1400 [==============================] - ETA: 6s - loss: 0.2549 - acc: 0.882 - ETA: 6s - loss: 0.2082 - acc: 0.906 - ETA: 5s - loss: 0.2117 - acc: 0.911 - ETA: 4s - loss: 0.1948 - acc: 0.923 - ETA: 4s - loss: 0.1981 - acc: 0.928 - ETA: 3s - loss: 0.1913 - acc: 0.932 - ETA: 2s - loss: 0.1863 - acc: 0.934 - ETA: 2s - loss: 0.1905 - acc: 0.931 - ETA: 1s - loss: 0.1908 - acc: 0.929 - ETA: 0s - loss: 0.1946 - acc: 0.926 - 9s 6ms/step - loss: 0.1934 - acc: 0.9264 - val_loss: 0.8206 - val_acc: 0.7100\n",
      "Epoch 12/15\n",
      "1400/1400 [==============================] - ETA: 6s - loss: 0.2401 - acc: 0.937 - ETA: 5s - loss: 0.1828 - acc: 0.945 - ETA: 5s - loss: 0.2036 - acc: 0.927 - ETA: 4s - loss: 0.1950 - acc: 0.931 - ETA: 3s - loss: 0.1990 - acc: 0.923 - ETA: 3s - loss: 0.1875 - acc: 0.928 - ETA: 2s - loss: 0.1856 - acc: 0.924 - ETA: 2s - loss: 0.1839 - acc: 0.925 - ETA: 1s - loss: 0.1845 - acc: 0.925 - ETA: 0s - loss: 0.1906 - acc: 0.922 - 9s 6ms/step - loss: 0.1879 - acc: 0.9250 - val_loss: 0.9206 - val_acc: 0.6267\n",
      "Epoch 13/15\n",
      "1400/1400 [==============================] - ETA: 6s - loss: 0.2023 - acc: 0.914 - ETA: 6s - loss: 0.2333 - acc: 0.898 - ETA: 5s - loss: 0.2322 - acc: 0.903 - ETA: 5s - loss: 0.2270 - acc: 0.906 - ETA: 4s - loss: 0.2198 - acc: 0.912 - ETA: 3s - loss: 0.2214 - acc: 0.914 - ETA: 2s - loss: 0.2111 - acc: 0.918 - ETA: 2s - loss: 0.2027 - acc: 0.922 - ETA: 1s - loss: 0.1975 - acc: 0.923 - ETA: 0s - loss: 0.2019 - acc: 0.923 - 9s 6ms/step - loss: 0.2018 - acc: 0.9243 - val_loss: 0.8758 - val_acc: 0.6733\n",
      "Epoch 14/15\n",
      "1400/1400 [==============================] - ETA: 6s - loss: 0.2617 - acc: 0.906 - ETA: 5s - loss: 0.2229 - acc: 0.906 - ETA: 5s - loss: 0.2371 - acc: 0.903 - ETA: 5s - loss: 0.2131 - acc: 0.916 - ETA: 4s - loss: 0.2049 - acc: 0.921 - ETA: 3s - loss: 0.1921 - acc: 0.924 - ETA: 2s - loss: 0.1868 - acc: 0.929 - ETA: 2s - loss: 0.1785 - acc: 0.933 - ETA: 1s - loss: 0.1802 - acc: 0.933 - ETA: 0s - loss: 0.1785 - acc: 0.932 - 9s 6ms/step - loss: 0.1791 - acc: 0.9329 - val_loss: 0.9696 - val_acc: 0.6533\n",
      "Epoch 15/15\n",
      "1400/1400 [==============================] - ETA: 6s - loss: 0.1550 - acc: 0.929 - ETA: 6s - loss: 0.1801 - acc: 0.929 - ETA: 5s - loss: 0.1691 - acc: 0.937 - ETA: 4s - loss: 0.1593 - acc: 0.943 - ETA: 4s - loss: 0.1678 - acc: 0.939 - ETA: 3s - loss: 0.1675 - acc: 0.938 - ETA: 2s - loss: 0.1644 - acc: 0.939 - ETA: 2s - loss: 0.1592 - acc: 0.941 - ETA: 1s - loss: 0.1674 - acc: 0.936 - ETA: 0s - loss: 0.1701 - acc: 0.935 - 9s 6ms/step - loss: 0.1669 - acc: 0.9386 - val_loss: 0.8144 - val_acc: 0.6750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e21100208>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(vocabulary)+1, output_dim = embedding_dim, weights =[embedding], trainable = False ))\n",
    "model.add(GRU(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print('Train....')\n",
    "model.fit(train_x, train_y, batch_size = 128, epochs =15, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array(vocabulary.doc2idx(wordlist[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in vocabulary[:2]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'synl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-137a6cefa0df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mword2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynl\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'synl'"
     ]
    }
   ],
   "source": [
    "word2v_model.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load vocabulary\n",
    "wordlist = []\n",
    "with open('vocabulary.txt', 'rb') as vocabulary:\n",
    "    wordlist = pickle.load(vocabulary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens - 5291\n"
     ]
    }
   ],
   "source": [
    "# Keeping track of frequency of a single token\n",
    "frequency = defaultdict(int)\n",
    "for text in wordlist:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "        \n",
    "# Apply Threshold to limit the vocabulary size, discarding the tokens which appeard number of times below the threshold limit \n",
    "FREQ_THRESHOLD = 5\n",
    "\n",
    "thresholded_wordlist =  [[token for token in text if frequency[token] > FREQ_THRESHOLD]\n",
    "          for text in wordlist]\n",
    "\n",
    "# Create Dictionary based on the word list\n",
    "dictionary = Dictionary(thresholded_wordlist)\n",
    "\n",
    "# Number of tokens\n",
    "print(\"Number of Tokens - {}\".format(len(dictionary))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://i.imgur.com/f1uzTDZ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* From the screenshot you can see the implementation of word-cooccurance matrix, based on the tokens from the dictionary, build a word-cooccurance matrix yourself which is $X$. Documentation of gensim [https://radimrehurek.com/gensim/corpora/dictionary.html]\n",
    "* Apply SVD on $X$\n",
    "* Reduce Dimension \n",
    "\n",
    "![dimen_reduc](https://i.imgur.com/lezB870.png)\n",
    "\n",
    "* Here Richard is taking only top two dimensions of the vector $U$, recommended size is *50* for now.\n",
    "\n",
    "![dimen_reduc_u](https://i.imgur.com/TA2Bmsq.png)\n",
    "\n",
    "* Now we can get a fixed size vector for each word. \n",
    "\n",
    "* Try to plot something similar based on the given dataset. In class we will try to implement a logistic regression classifier that can classify positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From Given corpus build Matrix\n",
    "len_dic = len(dictionary)\n",
    "X = np.zeros((len_dic,len_dic))\n",
    "la = np.linalg\n",
    "for review in thresholded_wordlist: \n",
    "    id = dictionary.doc2idx(review)\n",
    "    #print(id)\n",
    "    for i in id:\n",
    "        for j in id:\n",
    "            X[i,j] +=1\n",
    "U,s,Vh = la.svd(X, full_matrices=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5464, 50)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_sample = U[:,:50]\n",
    "U_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e009b1668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "for i in [2,100, 4, 56]:\n",
    "    plt.text(U[i,0], U[i,1], dictionary[i])\n",
    "    plt.axis(\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SentoVec(sentence):\n",
    "    #I like this movie. -> [I, like, this, movie] -> doc2idx -> [0, 1, 2, 3] -> U[0, 1, 2, 3], \n",
    "    #-> x1 = sum(vec) / len(sentence)\t\n",
    "    l = len(sentence)\n",
    "    id = dictionary.doc2idx(sentence)\n",
    "    vec = U_sample[id]\n",
    "    total_sum = sum(vec)\n",
    "    return total_sum\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prepare X_train using U vector and doc2idx from gensim corpora [use thresholded wordlist for this]\n",
    "X_train = []\n",
    "for review in thresholded_wordlist:\n",
    "    #Pass each senetence to the funtion and funtion returs probabilty\n",
    "    X_train.append(SentoVec(review)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 50)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.asarray(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = np.zeros(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prepare y_train by marking positive review as 1 and negative review as 0\n",
    "Y_train[:999] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train a scikit-learn LogisticRegression classifier using Input and Target Data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83699999999999997"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1000/1000 [11:59<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.concat((df_pos[DATA_LIMIT:1500], df_neg[DATA_LIMIT:1500])) #Test database\n",
    "if 1 != 0:\n",
    "    wordlist_test = []\n",
    "    for i in tqdm(range(df_test.shape[0])):\n",
    "        wordlist_test.append(process_text(df_test['review'].iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordlist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_text = []\n",
    "for review in wordlist_test:\n",
    "    X_test.append(word_vector(review))\n",
    "X_test = np.asarray(X_test)\n",
    "\n",
    "y_test = np.zeros(1000)\n",
    "y_test[:500] = 1\n",
    "\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = LR.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5464, 5464)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_2_id = dictionary.token2id\n",
    "#print(token)\n",
    "token_2_id['actor']\n",
    "X = np.zeros((len(dictionary), len(dictionary)))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for review in thresholded_wordlist:\n",
    "    for word in range(len(review)-1):\n",
    "        #if review[word] in token_2_id.keys() and review[word+1] in token_2_id.keys():\n",
    "            X[token_2_id[review[word]]][token_2_id[review[word+1]]] += 1\n",
    "            X[token_2_id[review[word+1]]][token_2_id[review[word]]] += 1\n",
    "            \n",
    "X[15][39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.zeros((len(dictionary), len(dictionary)))\n",
    "X.shape\n",
    "for list in thresholded_wordlist:\n",
    "    text_to_id = dictionary.doc2idx(list)\n",
    "\n",
    "    for i in text_to_id:\n",
    "        for j in text_to_id:\n",
    "            if(i==j):\n",
    "                X[i][j] = 0\n",
    "            X[i][j] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X[1][0]\n",
    "la = np.linalg\n",
    "U, s, Vh = la.svd(X, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sen2word(sentence):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U_New = U[:50]\n",
    "X_train = []\n",
    "for sentence in thresholded_wordlist:\n",
    "    sen2word(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(1, 500, 0, 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-86b2e5154e70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, tokenid)\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[1;31m# recompute id->word accordingly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid2token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrevdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid2token\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtokenid\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# will throw for non-existent ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: (1, 500, 0, 50)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "U.shape\n",
    "for i in dictionary[1, 500, 0, 50]:\n",
    "    plt.text(U[i,0], U[i,1], dictionary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
