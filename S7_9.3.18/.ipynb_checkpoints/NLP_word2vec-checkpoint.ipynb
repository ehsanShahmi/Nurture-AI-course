{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping track of frequency of a single token\n",
    "frequency = defaultdict(int)\n",
    "for text in wordlist:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "        \n",
    "# Apply Threshold to limit the vocabulary size, discarding the tokens which appeard number of times below the threshold limit \n",
    "FREQ_THRESHOLD = 5\n",
    "\n",
    "thresholded_wordlist =  [[token for token in text if frequency[token] > FREQ_THRESHOLD]\n",
    "          for text in wordlist]\n",
    "\n",
    "# Create Dictionary based on the word list\n",
    "dictionary = Dictionary(thresholded_wordlist)\n",
    "\n",
    "# Number of tokens\n",
    "print(\"Number of Tokens - {}\".format(len(dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make this statement true to run from scratch [It takes time to process the text]\n",
    "if 1 != 0:\n",
    "    wordlist = []\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        wordlist.append(process_text(df['review'].iloc[i]))\n",
    "        \n",
    "    with open('vocabulary.txt', 'wb') as vocabulary:\n",
    "        pickle.dump(wordlist, vocabulary)\n",
    "    vocabulary.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "DATA_LIMIT = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./imdb_master.csv', encoding='latin1')\n",
    "df_neg = df[df['label'] == 'neg']\n",
    "df_pos = df[df['label'] == 'pos']\n",
    "df = pd.concat((df_pos[:DATA_LIMIT], df_neg[:DATA_LIMIT]))\n",
    "\n",
    "def process_text(input_string, return_string=False, stem=False):\n",
    "    text = nlp(u'' + input_string)\n",
    "    if stem == True:\n",
    "        text = [tok.lemma_ for tok in text if (tok.is_alpha and not tok.is_stop)]\n",
    "    else:\n",
    "        text = [tok.lower_ for tok in text if (tok.is_alpha and not tok.is_stop)]\n",
    "    if return_string == True:\n",
    "        return \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load vocabulary\n",
    "wordlist = []\n",
    "with open('vocabulary.txt', 'rb') as vocabulary:\n",
    "    wordlist = pickle.load(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordlist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=0, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# Here we build just the skeleton model for the  word2vec, where we will fit the wordlist next.\n",
    "import multiprocessing\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "word2vec_model = Word2Vec(window = 5, workers = multiprocessing.cpu_count(), iter = 100, min_count =1, hs=1, negative=0)\n",
    "print (word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=22449, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# Now we fit in the wordlist into the skeleton model\n",
    "word2vec_model.build_vocab(wordlist)\n",
    "print (word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 29 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20549283, 21977100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we will see how much time needed to train\n",
    "%time word2vec_model.train(wordlist, total_examples=word2vec_model.corpus_count, epochs=word2vec_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WE test a single word if its shape (shape of the produced vector of this word) has right number of features\n",
    "word2vec_model.wv['cat'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('belucci', 0.44353532791137695),\n",
       " ('idolized', 0.3700087070465088),\n",
       " ('takagi', 0.3576205372810364),\n",
       " ('risk', 0.35502320528030396),\n",
       " ('romane', 0.3549274802207947),\n",
       " ('limos', 0.35447216033935547),\n",
       " ('revolving', 0.3485170900821686),\n",
       " ('sommerish', 0.34799548983573914),\n",
       " ('passages', 0.34680700302124023),\n",
       " ('prequels', 0.3446471691131592)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We test the similarities of a given word in the entire review, using these created vectors\n",
    "word2vec_model.wv.most_similar('bellucci')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movies', 0.43051135540008545),\n",
       " ('film', 0.423736572265625),\n",
       " ('undoubtably', 0.4197717607021332),\n",
       " ('paved', 0.3855682611465454),\n",
       " ('films', 0.3520013093948364),\n",
       " ('believe', 0.3511885106563568),\n",
       " ('rapped', 0.34690144658088684),\n",
       " ('immediately', 0.33001941442489624),\n",
       " ('heartening', 0.3207009434700012),\n",
       " ('cinema', 0.31249696016311646)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also test these similarities differentiating good/bad movies, like this:\n",
    "word2vec_model.wv.most_similar(positive=['movie', 'good'], negative=['bad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we move on to carry on these following tasks:\n",
    "1. Create a numpy empty/random matrix with dimension of [Voc size+1 X, Embedding dimension]\n",
    "2. Load the embeddings into that word\n",
    "3. Create keras embedding layer with the same configuration and load weights there\n",
    "4. Train a RNN/CNN to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22449"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We create a dictionary to store words to their indices\n",
    "vocabulary = Dictionary(wordlist)\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22450, 100)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tasks 1 & 2...\n",
    "# We create a matrix with the given dimensions...we declare the embedding dimension to be 100 as defualt\n",
    "# embedding dimensions show the no. of features of each word\n",
    "embedding_dim = 100\n",
    "embedding = np.zeros((len(vocabulary)+1, embedding_dim))\n",
    "\n",
    "# Now we fill up this embeddings into this embedding matrix:\n",
    "for i in range(len(vocabulary)):\n",
    "        embedding[i+1] = word2vec_model.wv[ vocabulary[i] ]\n",
    "embedding.shape                \n",
    "# This will be 1 more than the original vocabulary, since all the words are one element shifted to the right\n",
    "# for all the unknown tokens to be in the first cell of the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Tasks 3 & 4...\n",
    "# First we need keras to import and then create the embedding layer\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GRU, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To make all of these reviews equal length, we can either use padding, \n",
    "# or we can delete many words from each review to get an equal length for all the review...\n",
    "# We need padding so that all the reviews become of equal length\n",
    "# this will make training easier \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# We first create an un-padded training X...then we use padding\n",
    "X = []\n",
    "for review in wordlist:\n",
    "    X.append(np.array(vocabulary.doc2idx(review)) + 1)\n",
    "    \n",
    "train_x = pad_sequences(X, value = 0, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will show that each elemnt (here, elements are lists) will have equal length of 200\n",
    "len(train_x[0]), len(train_x[1599])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0, 4397,   31, 4378,  619, 4367,   17,  508,  372, 4388,  168,\n",
       "        507,  192,  278,  308, 1033,   79, 1527, 4369, 3898, 4269,  858,\n",
       "        406, 4125,  508, 1482,  312, 4377,  168,  973,  138,  308, 4389,\n",
       "        521, 4394,  493,  805,   44,  624,   44,  516,   44,  516,  964,\n",
       "        521,   31,   18,  411,  454,   31, 1163,  456, 4220,  203,  521,\n",
       "       4372, 4368,  219, 3627,  159, 4390, 4371,  268, 4370,  138,  308,\n",
       "       4397,  305,  473, 4373, 4395, 4380, 4376,  280, 4381, 4385, 4391,\n",
       "       4383,  611, 1050,  619, 1560,  113,  372, 4365, 1611, 2922,  307,\n",
       "        219, 1163, 3052, 1126, 2199,  342,  548,   31,  531, 4374,  249,\n",
       "       4375,  290, 4382,  290,  219, 1272, 4379,   98, 4396, 4393,  821,\n",
       "         61,  188, 2411, 4386, 4366,  493, 4392, 3518,  968, 4387, 4384,\n",
       "        585,  452,  567,  113, 1036, 3518,   31, 2814,  115, 1161,  579,\n",
       "       1179, 1449])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the other extra cells in each review are considered as 0s...this is the result of padding\n",
    "train_x[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We now train true labels of y similarly, as we did during svd\n",
    "train_y = np.concatenate((np.ones(1000), np.zeros(1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finally we first construct the embedding layer\n",
    "# the load weights\n",
    "# and lastly classify using CNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vocabulary)+1, output_dim = embedding_dim, weights =[embedding], trainable = False ))\n",
    "model.add(GRU(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train....\n",
      "Train on 1400 samples, validate on 600 samples\n",
      "Epoch 1/15\n",
      "1400/1400 [==============================] - ETA: 36s - loss: 0.7113 - acc: 0.46 - ETA: 22s - loss: 0.7157 - acc: 0.47 - ETA: 16s - loss: 0.7095 - acc: 0.48 - ETA: 13s - loss: 0.6966 - acc: 0.51 - ETA: 10s - loss: 0.6871 - acc: 0.55 - ETA: 8s - loss: 0.6778 - acc: 0.5755 - ETA: 6s - loss: 0.6716 - acc: 0.587 - ETA: 4s - loss: 0.6648 - acc: 0.599 - ETA: 2s - loss: 0.6585 - acc: 0.610 - ETA: 1s - loss: 0.6517 - acc: 0.616 - 18s 13ms/step - loss: 0.6449 - acc: 0.6279 - val_loss: 1.0361 - val_acc: 0.1417\n",
      "Epoch 2/15\n",
      "1400/1400 [==============================] - ETA: 11s - loss: 0.5621 - acc: 0.70 - ETA: 10s - loss: 0.5618 - acc: 0.71 - ETA: 9s - loss: 0.5558 - acc: 0.7057 - ETA: 8s - loss: 0.5615 - acc: 0.703 - ETA: 6s - loss: 0.5518 - acc: 0.714 - ETA: 5s - loss: 0.5537 - acc: 0.716 - ETA: 4s - loss: 0.5464 - acc: 0.723 - ETA: 3s - loss: 0.5398 - acc: 0.727 - ETA: 2s - loss: 0.5347 - acc: 0.730 - ETA: 1s - loss: 0.5340 - acc: 0.730 - 15s 11ms/step - loss: 0.5310 - acc: 0.7307 - val_loss: 1.1495 - val_acc: 0.2300\n",
      "Epoch 3/15\n",
      "1400/1400 [==============================] - ETA: 12s - loss: 0.4401 - acc: 0.82 - ETA: 11s - loss: 0.4671 - acc: 0.78 - ETA: 9s - loss: 0.4665 - acc: 0.7865 - ETA: 8s - loss: 0.4723 - acc: 0.787 - ETA: 7s - loss: 0.4702 - acc: 0.790 - ETA: 5s - loss: 0.4789 - acc: 0.783 - ETA: 4s - loss: 0.4687 - acc: 0.783 - ETA: 3s - loss: 0.4684 - acc: 0.784 - ETA: 2s - loss: 0.4635 - acc: 0.790 - ETA: 1s - loss: 0.4664 - acc: 0.787 - 15s 11ms/step - loss: 0.4596 - acc: 0.7914 - val_loss: 1.0411 - val_acc: 0.4067\n",
      "Epoch 4/15\n",
      "1400/1400 [==============================] - ETA: 10s - loss: 0.4610 - acc: 0.76 - ETA: 10s - loss: 0.4107 - acc: 0.81 - ETA: 9s - loss: 0.4122 - acc: 0.8047 - ETA: 8s - loss: 0.4149 - acc: 0.804 - ETA: 7s - loss: 0.4148 - acc: 0.810 - ETA: 5s - loss: 0.4022 - acc: 0.819 - ETA: 4s - loss: 0.3966 - acc: 0.822 - ETA: 3s - loss: 0.3913 - acc: 0.824 - ETA: 2s - loss: 0.3862 - acc: 0.824 - ETA: 1s - loss: 0.3813 - acc: 0.828 - 15s 11ms/step - loss: 0.3823 - acc: 0.8300 - val_loss: 1.1108 - val_acc: 0.5767\n",
      "Epoch 5/15\n",
      "1400/1400 [==============================] - ETA: 10s - loss: 0.2191 - acc: 0.92 - ETA: 9s - loss: 0.2721 - acc: 0.9062 - ETA: 8s - loss: 0.2864 - acc: 0.895 - ETA: 7s - loss: 0.2963 - acc: 0.892 - ETA: 6s - loss: 0.3052 - acc: 0.887 - ETA: 5s - loss: 0.3165 - acc: 0.880 - ETA: 4s - loss: 0.3246 - acc: 0.875 - ETA: 3s - loss: 0.3159 - acc: 0.880 - ETA: 2s - loss: 0.3238 - acc: 0.874 - ETA: 1s - loss: 0.3311 - acc: 0.870 - 14s 10ms/step - loss: 0.3375 - acc: 0.8657 - val_loss: 0.6604 - val_acc: 0.6783\n",
      "Epoch 6/15\n",
      "1400/1400 [==============================] - ETA: 12s - loss: 0.2850 - acc: 0.90 - ETA: 11s - loss: 0.3019 - acc: 0.90 - ETA: 9s - loss: 0.3137 - acc: 0.8854 - ETA: 8s - loss: 0.3154 - acc: 0.890 - ETA: 7s - loss: 0.3107 - acc: 0.885 - ETA: 6s - loss: 0.3129 - acc: 0.881 - ETA: 4s - loss: 0.3089 - acc: 0.879 - ETA: 3s - loss: 0.3071 - acc: 0.878 - ETA: 2s - loss: 0.3115 - acc: 0.874 - ETA: 1s - loss: 0.3070 - acc: 0.874 - 16s 11ms/step - loss: 0.3070 - acc: 0.8757 - val_loss: 0.8687 - val_acc: 0.5767\n",
      "Epoch 7/15\n",
      "1400/1400 [==============================] - ETA: 12s - loss: 0.2575 - acc: 0.90 - ETA: 11s - loss: 0.2809 - acc: 0.89 - ETA: 9s - loss: 0.2635 - acc: 0.9010 - ETA: 8s - loss: 0.2622 - acc: 0.900 - ETA: 7s - loss: 0.2580 - acc: 0.903 - ETA: 6s - loss: 0.2557 - acc: 0.902 - ETA: 4s - loss: 0.2531 - acc: 0.904 - ETA: 3s - loss: 0.2588 - acc: 0.902 - ETA: 2s - loss: 0.2673 - acc: 0.901 - ETA: 1s - loss: 0.2671 - acc: 0.901 - 16s 11ms/step - loss: 0.2679 - acc: 0.9014 - val_loss: 0.9984 - val_acc: 0.5817\n",
      "Epoch 8/15\n",
      "1400/1400 [==============================] - ETA: 12s - loss: 0.2326 - acc: 0.87 - ETA: 11s - loss: 0.2335 - acc: 0.88 - ETA: 9s - loss: 0.2490 - acc: 0.8906 - ETA: 8s - loss: 0.2692 - acc: 0.884 - ETA: 7s - loss: 0.2581 - acc: 0.890 - ETA: 6s - loss: 0.2666 - acc: 0.886 - ETA: 4s - loss: 0.2606 - acc: 0.887 - ETA: 3s - loss: 0.2686 - acc: 0.883 - ETA: 2s - loss: 0.2845 - acc: 0.883 - ETA: 1s - loss: 0.2798 - acc: 0.885 - 15s 11ms/step - loss: 0.2709 - acc: 0.8907 - val_loss: 0.9326 - val_acc: 0.6183\n",
      "Epoch 9/15\n",
      "1400/1400 [==============================] - ETA: 10s - loss: 0.2162 - acc: 0.90 - ETA: 9s - loss: 0.2566 - acc: 0.9023 - ETA: 8s - loss: 0.2585 - acc: 0.895 - ETA: 7s - loss: 0.2532 - acc: 0.910 - ETA: 6s - loss: 0.2479 - acc: 0.912 - ETA: 5s - loss: 0.2378 - acc: 0.916 - ETA: 4s - loss: 0.2352 - acc: 0.918 - ETA: 3s - loss: 0.2369 - acc: 0.917 - ETA: 2s - loss: 0.2326 - acc: 0.918 - ETA: 1s - loss: 0.2316 - acc: 0.918 - 14s 10ms/step - loss: 0.2337 - acc: 0.9157 - val_loss: 0.9820 - val_acc: 0.5817\n",
      "Epoch 10/15\n",
      "1400/1400 [==============================] - ETA: 10s - loss: 0.3058 - acc: 0.87 - ETA: 9s - loss: 0.2966 - acc: 0.8633 - ETA: 8s - loss: 0.2590 - acc: 0.890 - ETA: 7s - loss: 0.2713 - acc: 0.890 - ETA: 6s - loss: 0.2477 - acc: 0.901 - ETA: 5s - loss: 0.2531 - acc: 0.899 - ETA: 4s - loss: 0.2397 - acc: 0.907 - ETA: 3s - loss: 0.2346 - acc: 0.909 - ETA: 2s - loss: 0.2300 - acc: 0.912 - ETA: 1s - loss: 0.2237 - acc: 0.915 - 14s 10ms/step - loss: 0.2206 - acc: 0.9171 - val_loss: 0.8699 - val_acc: 0.6600\n",
      "Epoch 11/15\n",
      "1400/1400 [==============================] - ETA: 11s - loss: 0.2376 - acc: 0.89 - ETA: 10s - loss: 0.1832 - acc: 0.93 - ETA: 8s - loss: 0.2153 - acc: 0.9219 - ETA: 7s - loss: 0.2181 - acc: 0.925 - ETA: 6s - loss: 0.2153 - acc: 0.925 - ETA: 5s - loss: 0.2088 - acc: 0.928 - ETA: 4s - loss: 0.2045 - acc: 0.928 - ETA: 3s - loss: 0.2010 - acc: 0.929 - ETA: 2s - loss: 0.1961 - acc: 0.929 - ETA: 1s - loss: 0.2001 - acc: 0.928 - 14s 10ms/step - loss: 0.1979 - acc: 0.9293 - val_loss: 0.9488 - val_acc: 0.6717\n",
      "Epoch 12/15\n",
      "1400/1400 [==============================] - ETA: 11s - loss: 0.3195 - acc: 0.90 - ETA: 10s - loss: 0.2456 - acc: 0.93 - ETA: 8s - loss: 0.2210 - acc: 0.9375 - ETA: 7s - loss: 0.2368 - acc: 0.929 - ETA: 6s - loss: 0.2594 - acc: 0.920 - ETA: 5s - loss: 0.2538 - acc: 0.921 - ETA: 4s - loss: 0.2757 - acc: 0.916 - ETA: 3s - loss: 0.2747 - acc: 0.915 - ETA: 2s - loss: 0.2645 - acc: 0.916 - ETA: 1s - loss: 0.2580 - acc: 0.918 - 14s 10ms/step - loss: 0.2500 - acc: 0.9221 - val_loss: 1.0638 - val_acc: 0.5317\n",
      "Epoch 13/15\n",
      "1400/1400 [==============================] - ETA: 10s - loss: 0.1881 - acc: 0.93 - ETA: 9s - loss: 0.1903 - acc: 0.9375 - ETA: 8s - loss: 0.2050 - acc: 0.929 - ETA: 7s - loss: 0.2146 - acc: 0.927 - ETA: 6s - loss: 0.2100 - acc: 0.928 - ETA: 5s - loss: 0.2149 - acc: 0.920 - ETA: 4s - loss: 0.2173 - acc: 0.918 - ETA: 3s - loss: 0.2136 - acc: 0.922 - ETA: 2s - loss: 0.2152 - acc: 0.921 - ETA: 1s - loss: 0.2116 - acc: 0.925 - 14s 10ms/step - loss: 0.2066 - acc: 0.9293 - val_loss: 0.9780 - val_acc: 0.5817\n",
      "Epoch 14/15\n",
      "1400/1400 [==============================] - ETA: 11s - loss: 0.1848 - acc: 0.96 - ETA: 10s - loss: 0.1637 - acc: 0.95 - ETA: 8s - loss: 0.1637 - acc: 0.9583 - ETA: 7s - loss: 0.1595 - acc: 0.955 - ETA: 6s - loss: 0.1783 - acc: 0.948 - ETA: 5s - loss: 0.1804 - acc: 0.944 - ETA: 4s - loss: 0.1739 - acc: 0.947 - ETA: 3s - loss: 0.1806 - acc: 0.944 - ETA: 2s - loss: 0.1786 - acc: 0.945 - ETA: 1s - loss: 0.1737 - acc: 0.947 - 14s 10ms/step - loss: 0.1739 - acc: 0.9457 - val_loss: 0.8079 - val_acc: 0.7133\n",
      "Epoch 15/15\n",
      "1400/1400 [==============================] - ETA: 11s - loss: 0.1884 - acc: 0.94 - ETA: 9s - loss: 0.1943 - acc: 0.9297 - ETA: 8s - loss: 0.1965 - acc: 0.924 - ETA: 7s - loss: 0.1885 - acc: 0.933 - ETA: 6s - loss: 0.1920 - acc: 0.931 - ETA: 5s - loss: 0.1898 - acc: 0.929 - ETA: 4s - loss: 0.1839 - acc: 0.928 - ETA: 3s - loss: 0.1815 - acc: 0.930 - ETA: 2s - loss: 0.1792 - acc: 0.930 - ETA: 1s - loss: 0.1793 - acc: 0.930 - 14s 10ms/step - loss: 0.1768 - acc: 0.9314 - val_loss: 1.1787 - val_acc: 0.5900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2aa3a484c50>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# at last, we compile and then we fit using our created CNN classifier\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print('Train....')\n",
    "model.fit(train_x, train_y, batch_size = 128, epochs =15, validation_split=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
