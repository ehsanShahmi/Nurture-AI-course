{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this block to install dependencies [Remember to make the statement true]\n",
    "if 0 != 1:\n",
    "    !pip install pandas\n",
    "    !pip install tqdm\n",
    "    !pip install scikit-learn\n",
    "    !pip install gensim\n",
    "    !pip install spacy\n",
    "    !python -m spacy download en\n",
    "    !pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 2000/2000 [00:54<00:00, 36.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# Make this statement true to run from scratch [It takes time to process the text]\n",
    "if 1 != 0:\n",
    "    wordlist = []\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        wordlist.append(process_text(df['review'].iloc[i]))\n",
    "        \n",
    "    with open('vocabulary.txt', 'wb') as vocabulary:\n",
    "        pickle.dump(wordlist, vocabulary)\n",
    "    vocabulary.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "DATA_LIMIT = 1000\n",
    "\n",
    "df = pd.read_csv('./imdb_master.csv', encoding='latin1')\n",
    "df_neg = df[df['label'] == 'neg']\n",
    "df_pos = df[df['label'] == 'pos']\n",
    "df = pd.concat((df_pos[:DATA_LIMIT], df_neg[:DATA_LIMIT]))\n",
    "\n",
    "def process_text(input_string, return_string=False, stem=False):\n",
    "    text = nlp(u'' + input_string)\n",
    "    if stem == True:\n",
    "        text = [tok.lemma_ for tok in text if (tok.is_alpha and not tok.is_stop)]\n",
    "    else:\n",
    "        text = [tok.lower_ for tok in text if (tok.is_alpha and not tok.is_stop)]\n",
    "    if return_string == True:\n",
    "        return \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12500</th>\n",
       "      <td>12500</td>\n",
       "      <td>test</td>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0_10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12501</th>\n",
       "      <td>12501</td>\n",
       "      <td>test</td>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>pos</td>\n",
       "      <td>10000_7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12502</th>\n",
       "      <td>12502</td>\n",
       "      <td>test</td>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>pos</td>\n",
       "      <td>10001_9.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12503</th>\n",
       "      <td>12503</td>\n",
       "      <td>test</td>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>10002_8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12504</th>\n",
       "      <td>12504</td>\n",
       "      <td>test</td>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>pos</td>\n",
       "      <td>10003_8.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  type                                             review  \\\n",
       "12500       12500  test  I went and saw this movie last night after bei...   \n",
       "12501       12501  test  Actor turned director Bill Paxton follows up h...   \n",
       "12502       12502  test  As a recreational golfer with some knowledge o...   \n",
       "12503       12503  test  I saw this film in a sneak preview, and it is ...   \n",
       "12504       12504  test  Bill Paxton has taken the true story of the 19...   \n",
       "\n",
       "      label         file  \n",
       "12500   pos     0_10.txt  \n",
       "12501   pos  10000_7.txt  \n",
       "12502   pos  10001_9.txt  \n",
       "12503   pos  10002_8.txt  \n",
       "12504   pos  10003_8.txt  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load vocabulary\n",
    "wordlist = []\n",
    "with open('vocabulary.txt', 'rb') as vocabulary:\n",
    "    wordlist = pickle.load(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordlist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens - 5464\n"
     ]
    }
   ],
   "source": [
    "# Keeping track of frequency of a single token\n",
    "frequency = defaultdict(int)\n",
    "for text in wordlist:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "        \n",
    "# Apply Threshold to limit the vocabulary size, discarding the tokens which appeard number of times below the threshold limit \n",
    "FREQ_THRESHOLD = 5\n",
    "\n",
    "thresholded_wordlist =  [[token for token in text if frequency[token] > FREQ_THRESHOLD]\n",
    "          for text in wordlist]\n",
    "\n",
    "# Create Dictionary based on the word list\n",
    "dictionary = Dictionary(thresholded_wordlist)\n",
    "\n",
    "# Number of tokens\n",
    "print(\"Number of Tokens - {}\".format(len(dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'admit'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(thresholded_wordlist[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://i.imgur.com/f1uzTDZ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* From the screenshot you can see the implementation of word-cooccurance matrix, based on the tokens from the dictionary, build a word-cooccurance matrix yourself which is $X$. Documentation of gensim [https://radimrehurek.com/gensim/corpora/dictionary.html]\n",
    "* Apply SVD on $X$\n",
    "* Reduce Dimension \n",
    "\n",
    "![dimen_reduc](https://i.imgur.com/lezB870.png)\n",
    "\n",
    "* Here Richard is taking only top two dimensions of the vector $U$, recommended size is *50* for now.\n",
    "\n",
    "![dimen_reduc_u](https://i.imgur.com/TA2Bmsq.png)\n",
    "\n",
    "* Now we can get a fixed size vector for each word. \n",
    "\n",
    "* Try to plot something similar based on the given dataset. In class we will try to implement a logistic regression classifier that can classify positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5464, 5464)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constructing the co-occurance matrix, X\n",
    "X = np.zeros((len(dictionary),len(dictionary)))\n",
    "\n",
    "for list in thresholded_wordlist:\n",
    "    text_2_id = dictionary.doc2idx(list)\n",
    "    for i in text_2_id:\n",
    "        for j in text_2_id:\n",
    "            if i==j:\n",
    "                X[i][j] = 0\n",
    "            X[i][j] += 1\n",
    "\n",
    "X.shape                                         # To see if X is produced right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5464, 5464)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying svd and then getting U\n",
    "la = np.linalg\n",
    "U, s, Vh = la.svd(X, full_matrices = False)\n",
    "U.shape                                         # To see if svd has worked accurately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFMZJREFUeJzt3H2MVvWZ//H3BWjrA3WwUouCv8Eu1QFUkJFC3ET41SWA\nKJq18dkiqURr027atLK1if2nqa52lxJpKbU+d2tsdbvYYlyxYtWqZVCggrqOdLY+YEUrrlUaRa79\nY24m88XBebhvZgZ4v5I7c875Xuec6+tt5jP3OecmMhNJkrYb0NcNSJL6F4NBklQwGCRJBYNBklQw\nGCRJBYNBklQwGCRJBYNBklQwGCRJhUF93UBPHHLIIVlfX9/XbUjSbmXVqlWvZebQzup2y2Cor6+n\nqampr9uQpN1KRPxPV+q8lCRJKhgMkqSCwSBJKhgMkqSCwSBJKhgMkqSCwSBJKhgMkqSCwSBJKhgM\nkqSCwSBJKhgMkqSCwSBJKhgMkqSCwSBJKhgMkqSCwSBJKhgMkqSCwSBJKhgMkqSCwSBJKhgMkqSC\nwSBJKhgMkqRCTYIhIqZHxLMR0RwR8zsYj4hYWBlfGxHH7zA+MCKejIhf1aIfSVLPVR0METEQWATM\nAEYD50TE6B3KZgCjKq95wA93GP8K8HS1vUiSqleLTwwTgebM3JCZ7wK3A7N3qJkN3JKtHgPqImIY\nQEQMB04Brq9BL5KkKtUiGA4HXmi3/mJlW1drFgDfALbVoBdJUpX69OZzRMwCXs3MVV2onRcRTRHR\ntGnTpl7oTpL2TrUIhpeAEe3Wh1e2daXmROC0iGih9RLU/4+I2zo6SWYuyczGzGwcOnRoDdqWJHWk\nFsGwEhgVESMjYl/gbGDpDjVLgQsrTydNAt7MzI2Z+c+ZOTwz6yv7/SYzz69BT5KkHhpU7QEyc2tE\nfAm4FxgI3JCZ6yLiksr4YmAZMBNoBt4BLqr2vJKkXSMys6976LbGxsZsamrq6zYkabcSEasys7Gz\nOr/5LEkqGAySpILBIEkqGAySpILBIEkqGAySpILBIEkqGAySpILBIEkqGAySpILBIEkqGAySpILB\nIEkqGAySpILBIEkqGAySpILBIEkqGAySpILBIEkqGAySpILBIEkqGAySpILBIEkqGAySpILBIEkq\nGAySpILBIEkqGAySpILBIEkqGAySpILBIEkq1CQYImJ6RDwbEc0RMb+D8YiIhZXxtRFxfGX7iIh4\nICLWR8S6iPhKLfqRJPVc1cEQEQOBRcAMYDRwTkSM3qFsBjCq8poH/LCyfSvwtcwcDUwCLutgX0lS\nL6rFJ4aJQHNmbsjMd4Hbgdk71MwGbslWjwF1ETEsMzdm5hMAmfkW8DRweA16kiT1UC2C4XDghXbr\nL/LBX+6d1kREPTAeeLwGPUmSeqhf3HyOiAOBO4F/ysz/3UnNvIhoioimTZs29W6DkrQXqUUwvASM\naLc+vLKtSzURsQ+tofDTzLxrZyfJzCWZ2ZiZjUOHDq1B25KkjtQiGFYCoyJiZETsC5wNLN2hZilw\nYeXppEnAm5m5MSIC+AnwdGb+aw16kSRVaVC1B8jMrRHxJeBeYCBwQ2aui4hLKuOLgWXATKAZeAe4\nqLL7icAFwB8iYnVl2zczc1m1fUmSeiYys6976LbGxsZsamrq6zYkabcSEasys7Gzun5x81mS1H8Y\nDJKkgsEgSSoYDJKkgsEgSSoYDJKkgsEgSSoYDJKkgsEgSSoYDJKkgsEgSSoYDJKkgsEgSSoYDJKk\ngsEgSSoYDJKkgsEgSSoYDJKkgsEgSSoYDJKkgsEgSSoYDJKkgsEgSSoYDJKkgsEgSSoYDJKkgsEg\nSSoYDJKkgsEgSSoYDFI7LS0tjB07tstjc+bM4Re/+MUHalesWMGsWbO6ff4FCxbwzjvvtK3PnDmT\nzZs3d/s4UjVqEgwRMT0ino2I5oiY38F4RMTCyvjaiDi+q/tKe5Mdg2HZsmXU1dX1YUfaG1UdDBEx\nEFgEzABGA+dExOgdymYAoyqvecAPu7Gv1Kvef/99Lr74YsaMGcO0adPYsmULq1ev5owzzqC5uZn6\n+nqOPvpopk2bxvvvv8/y5cs54YQTOPLIIxk8eDDjxo3jrrvuYs2aNcWniQMPPBBo/TQxZcoUzjzz\nTI4++mjOO+88MpOFCxfy8ssvM3XqVKZOnQpAfX09r732Gi0tLTQ0NHygL4CVK1dy7LHHMm7cOL7+\n9a/v9BOP1FW1+MQwEWjOzA2Z+S5wOzB7h5rZwC3Z6jGgLiKGdXFfqVc999xzXHbZZaxbt466ujru\nvPNOLrzwQi6//HK2bt3KtGnTmD59OnV1dbS0tPCZz3yGhx56iPfee48LLriAuXPn8sorr3zoOZ58\n8kkWLFjA+vXr2bBhA4888ghf/vKXOeyww3jggQd44IEHutQXwEUXXcSPfvQjVq9ezcCBA3fJfxPt\nXWoRDIcDL7Rbf7GyrSs1XdlX6lUjR45k3LhxAEyYMIHnn3+ezZs3M2nSJEaOHMnll1/Ob3/7WyZM\nmMDbb7/Nn/70JyZPnsxf/vIX7rnnHtavX8/555//oeeYOHEiw4cPZ8CAAYwbN46WlpZu99XS0sLm\nzZt56623mDx5MgDnnntudZOX2I1uPkfEvIhoioimTZs29XU72oN95CMfaVseOHBgcfN3x7Ft27ax\naNEi5s+fz4QJE7jyyiv529/+BkBEsG3bNgC2bdvGu+++u9PjbN26tdt9dWUfqSdqEQwvASParQ+v\nbOtKTVf2BSAzl2RmY2Y2Dh06tOqmpa466KCDGDJkCL///e8BuPXWWznppJPaxrds2cLkyZNpaWnh\nxz/+MQA/+9nP2G+//Vi1ahUAS5cu5b333uv0XIMHD+att97qcm91dXUMHjyYxx9/HIDbb7+9y/tK\nO1OLYFgJjIqIkRGxL3A2sHSHmqXAhZWnkyYBb2bmxi7uK/W5m2++me9+97s0NzezdOnS4pf32Wef\nzUknncR+++3H+vXrufvuu/nEJz7BEUccwYMPPshxxx3HV7/6VQ444AA2b97ML3/5y52eZ968eUyf\nPr3t5nNX/OQnP+Hiiy9m3LhxvP322xx00EFVzVWKzKz+IBEzgQXAQOCGzPxORFwCkJmLIyKA64Dp\nwDvARZnZtLN9OztfY2NjNjU1Vd231BM33XQTTU1NXHfddd3et6WlhVmzZvHUU0/VrJ+//vWvbU88\nXXXVVWzcuJHvf//7NTu+9hwRsSozGzurq8k9hsxclpmfzsxPbf/FnpmLM3NxZTkz87LK+DHbQ2Fn\n+0odmTJlCr3xB8Hpp5/OhAkTGDNmDEuWLAHgxhtv5NOf/jQTJ07kkUceaaudM2cOl156KZMmTeLI\nI49kxYoVzJ07l4aGBubMmdNWt/2x0/nz5/P888+3PVpaC7/+9a8ZN24cY8eO5aGHHuJb3/pWTY6r\nvdegvm5A6m9uuOEGDj74YLZs2cIJJ5zAKaecwpVXXsmqVas46KCDmDp1KuPHj2+rf+ONN3j00UdZ\nunQpp512Go888gjXX389J5xwAqtXr257kgha/6J/6qmnWL16dc36PeusszjrrLNqdjxpt3kqSXuP\nlpaWti9+NTQ0cOaZZxbfBga49NJLaWxsZMyYMVx55ZUA/OY3v+H0009vq7nvvvs444wzun3+hQsX\nctxxxzFp0iReeOEFbr31VqZMmcLQoUPZd999P/BL+NRTTyUiOOaYYzj00EM55phjGDBgAGPGjOnS\nY6hSf2MwqF969tln+eIXv8jTTz/Nxz72MX7wgx8U49/5zndoampi7dq1PPjgg6xdu5apU6fyzDPP\nsP1x5htvvJG5c+d267wrVqxg+fLlPProo6xZs4bx48dz9NFHf+g+2x8jHTBgQPFI6YABA3ykVLsl\ng0H90ogRIzjxxBMBOP/883n44YeL8TvuuIPjjz+e8ePHs27dOtavX09EcMEFF3DbbbexefNmHn30\nUWbMmNGt87755psMGTKE/fffn2eeeYbHHnuMLVu28OCDD/L666/z3nvv8fOf/7zH8+ru46hSX/Ae\ng/ql1gfZOl7/4x//yLXXXsvKlSsZMmQIc+bMaftS2UUXXcSpp57KRz/6UT73uc8xaFD3/hefPn06\nixcvpqGhgaOOOopJkyYxbNgwvv3tbzN58mTq6uqKewbd9fGPf5wTTzyRsWPHMmPGDK655poeH0va\nVWryuGpv83HVPVtLSwsjR47kd7/7HZMnT+YLX/gCDQ0N3H333Vx77bXss88+XHjhhTz55JNs2rSJ\nY489lquvvrrtKaBTTz2VJ554guXLl9PQ0NC3k5H6kV59XFWqtaOOOopFixbR0NDAG2+8waWXXto2\ndtxxx7Vd+z/33HPbLjltd9555zFixAhDQeohLyWpXxo0aBC33XZbsW3FihVtyzfddNNO93344Ye5\n+OKLd1Fn0p7PYNAeZcKECRxwwAF873vf6+tWpN2WwaB+p76+vsf/ZMT2f7ROUs95j0GSVDAYJEkF\ng0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GS\nVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVKgqGCLi4Ii4LyKeq/wcspO66RHxbEQ0R8T8dtuv\niYhnImJtRPxHRNRV048kqXrVfmKYD9yfmaOA+yvrhYgYCCwCZgCjgXMiYnRl+D5gbGYeC/w38M9V\n9iNJqlK1wTAbuLmyfDNwegc1E4HmzNyQme8Ct1f2IzP/KzO3VuoeA4ZX2Y8kqUrVBsOhmbmxsvwK\ncGgHNYcDL7Rbf7GybUdzgXuq7EeSVKVBnRVExHLgkx0MXdF+JTMzIrInTUTEFcBW4KcfUjMPmAdw\nxBFH9OQ0kqQu6DQYMvPknY1FxJ8jYlhmboyIYcCrHZS9BIxotz68sm37MeYAs4DPZuZOgyUzlwBL\nABobG3sUQJKkzlV7KWkp8PnK8ueB/+ygZiUwKiJGRsS+wNmV/YiI6cA3gNMy850qe5Ek1UC1wXAV\n8A8R8RxwcmWdiDgsIpYBVG4ufwm4F3gauCMz11X2vw4YDNwXEasjYnGV/UiSqtTppaQPk5mvA5/t\nYPvLwMx268uAZR3U/V0155ck1Z7ffJYkFQwGSVLBYJAkFQwGSVLBYJAkFQwGSVLBYJAkFQwGSVLB\nYJAkFQwGSVLBYJAkFQwGSVLBYJAkFQwGSVLBYJAkFQwGSVLBYJAkFQwGSVLBYJAkFQwGSVLBYJAk\nFQwGSVLBYJAkFQwGSVLBYJAkFQwGSVLBYJAkFQwGSVLBYJAkFQwGSVKhqmCIiIMj4r6IeK7yc8hO\n6qZHxLMR0RwR8zsY/1pEZEQcUk0/kqTqVfuJYT5wf2aOAu6vrBciYiCwCJgBjAbOiYjR7cZHANOA\nP1XZiySpBqoNhtnAzZXlm4HTO6iZCDRn5obMfBe4vbLfdv8GfAPIKnuRJNVAtcFwaGZurCy/Ahza\nQc3hwAvt1l+sbCMiZgMvZeaaKvuQJNXIoM4KImI58MkOhq5ov5KZGRFd/qs/IvYHvknrZaSu1M8D\n5gEcccQRXT2NJKmbOg2GzDx5Z2MR8eeIGJaZGyNiGPBqB2UvASParQ+vbPsUMBJYExHbtz8RERMz\n85UO+lgCLAFobGz0spMk7SLVXkpaCny+svx54D87qFkJjIqIkRGxL3A2sDQz/5CZn8jM+sysp/US\n0/EdhYIkqfdUGwxXAf8QEc8BJ1fWiYjDImIZQGZuBb4E3As8DdyRmeuqPK8kaRfp9FLSh8nM14HP\ndrD9ZWBmu/VlwLJOjlVfTS+SpNrwm8+SpILBIEkqGAySpILBIEkqGAySpILBIEkqGAySpILBIEkq\nGAySpILBIEkqGAySpILBIEkqGAySpILBIEkqGAySpILBIEkqGAySpILBIEkqGAySpILBIEkqGAyS\npILBIEkqGAySpILBIEkqGAySpEJkZl/30G0RsQn4n77uowcOAV7r6yZ60d42X3DOe4vddc7/LzOH\ndla0WwbD7ioimjKzsa/76C1723zBOe8t9vQ5eylJklQwGCRJBYOhdy3p6wZ62d42X3DOe4s9es7e\nY5AkFfzEIEkqGAw1FBEHR8R9EfFc5eeQndRNj4hnI6I5IuZ3MP61iMiIOGTXd12dauccEddExDMR\nsTYi/iMi6nqv++7pwvsWEbGwMr42Io7v6r79VU/nHBEjIuKBiFgfEesi4iu9333PVPM+V8YHRsST\nEfGr3uu6xjLTV41ewL8A8yvL84GrO6gZCDwPHAnsC6wBRrcbHwHcS+v3NA7p6znt6jkD04BBleWr\nO9q/P7w6e98qNTOBe4AAJgGPd3Xf/viqcs7DgOMry4OB/97T59xu/KvAvwO/6uv59PTlJ4bamg3c\nXFm+GTi9g5qJQHNmbsjMd4HbK/tt92/AN4Dd5eZPVXPOzP/KzK2VuseA4bu4357q7H2jsn5LtnoM\nqIuIYV3ctz/q8Zwzc2NmPgGQmW8BTwOH92bzPVTN+0xEDAdOAa7vzaZrzWCorUMzc2Nl+RXg0A5q\nDgdeaLf+YmUbETEbeCkz1+zSLmurqjnvYC6tf4n1R12Zw85qujr//qaaObeJiHpgPPB4zTusvWrn\nvIDWP+y27aoGe8Ogvm5gdxMRy4FPdjB0RfuVzMyI6PJf/RGxP/BNWi+t9Cu7as47nOMKYCvw057s\nr/4pIg4E7gT+KTP/t6/72ZUiYhbwamauiogpfd1PNQyGbsrMk3c2FhF/3v4xuvLR8tUOyl6i9T7C\ndsMr2z4FjATWRMT27U9ExMTMfKVmE+iBXTjn7ceYA8wCPpuVi7T90IfOoZOafbqwb39UzZyJiH1o\nDYWfZuZdu7DPWqpmzv8InBYRM4GPAh+LiNsy8/xd2O+u0dc3OfakF3AN5Y3Yf+mgZhCwgdYQ2H5z\na0wHdS3sHjefq5ozMB1YDwzt67l0Ms9O3zdary23vyn5++685/3tVeWcA7gFWNDX8+itOe9QM4Xd\n+OZznzewJ72AjwP3A88By4GDK9sPA5a1q5tJ61MazwNX7ORYu0swVDVnoJnW67WrK6/FfT2nD5nr\nB+YAXAJcUlkOYFFl/A9AY3fe8/746umcgb+n9QGKte3e25l9PZ9d/T63O8ZuHQx+81mSVPCpJElS\nwWCQJBUMBklSwWCQJBUMBklSwWCQJBUMBklSwWCQJBX+DxXTqG5T2cabAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20bc7905710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in [1, 140, 400, 750]:\n",
    "    plt.text(U[i,0], U[i,1], dictionary[i])\n",
    "    plt.axis(\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks after generating the 2d plot of U [THIS PROCEDURE ALSO WORKS FOR WORD2VEC] :\n",
    "\n",
    "\"I like this movie\" -> [I, like, this, movie] -> doc2idx -> [0, 1, 2, 3] -> U[0, 1, 2, 3], -> x1 = sum(vec) / len(sentence)\t\n",
    "\n",
    "1. Input & Target Data:\n",
    "    a. Prepare X_train using U vector and doc2idx from gensim corpora [use thresholded wordlist for this]\n",
    "    b. Prepare y_train by marking positive review as 1 and negative review as 0\n",
    "2. Train a scikit-learn LogisticRegression classifier using Input and Target Data\n",
    "3. Measure accuracy\n",
    "4. Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5464, 50)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Truncating to just 50 features of U\n",
    "U_new = U[:,:50] \n",
    "U_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to take in a review and convert into a list of vectors for the words in the review\n",
    "def word_vector(review):\n",
    "    index_list = dictionary.doc2idx(review)         # The list of the indices of the words from the review in the dictionary\n",
    "    unnormalized_word_vector = U_new[index_list]    # All the word vectors in U_new corresponding to each word of the review\n",
    "    word_vector = sum(unnormalized_word_vector)  # Just normalizing the vectors to retain all the features\n",
    "    #print (word_vector.shape)\n",
    "    return word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 50), (2000,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constructing the training data \n",
    "X_train = []\n",
    "for review in thresholded_wordlist:\n",
    "    X_train.append(word_vector(review))\n",
    "    \n",
    "X_train = np.asarray(X_train)\n",
    "\n",
    "y_train = np.zeros(2000)\n",
    "y_train[:1000] = 1                     # Keeping first half of y as positive 1, and rest as zero; \n",
    "                                       # As first half ar positive review, second half are negative review\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86399999999999999"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now applying the logistric regression on these training data and then testing accuracy of the training data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train, y_train)         # Training with training data\n",
    "LR.score(X_train, y_train)       # Testing accuracy of this training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 1000/1000 [00:24<00:00, 41.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# Applying similar procedure for test data.\n",
    "# First we need to create the test data wordlist from test database:\n",
    "\n",
    "df_test = pd.concat((df_pos[DATA_LIMIT:1500], df_neg[DATA_LIMIT:1500]))  # Test database\n",
    "if 1 != 0:\n",
    "    wordlist_test = []\n",
    "    for i in tqdm(range(df_test.shape[0])):\n",
    "        wordlist_test.append(process_text(df_test['review'].iloc[i]))    # Test dataset created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordlist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 50), (1000,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Secondly, we create the test data, X_test and y_test\n",
    "X_test = []\n",
    "for review in wordlist_test:\n",
    "    X_test.append(word_vector(review))\n",
    "X_test = np.asarray(X_test)\n",
    "\n",
    "y_test = np.zeros(1000)\n",
    "y_test[:500] = 1\n",
    "\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73999999999999999"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lastly finding out the score of the test data with accuracy\n",
    "\n",
    "LR.score(X_test, y_test)   # In previous assignments, we calculated accuracies by hand by counting and dividing\n",
    "                           # score() function does this by itself actually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[420,  80],\n",
       "       [180, 320]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = LR.predict(X_test)                     # This is required to get the confusion matrix\n",
    "# Lastly we make a confusioin matrix to see the true positives and false negatives\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
