{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this block to install dependencies [Remember to make the statement true]\n",
    "if 0 == 1:\n",
    "    !pip3 install pandas\n",
    "    !pip3 install tqdm\n",
    "    !pip3 install scikit-learn\n",
    "    !pip3 install gensim\n",
    "    !pip3 install spacy\n",
    "    !python3 -m spacy download en\n",
    "    !pip3 install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "DATA_LIMIT = 1000\n",
    "\n",
    "df = pd.read_csv('./imdb_master.csv', encoding='latin1')\n",
    "df_neg = df[df['label'] == 'neg']\n",
    "df_pos = df[df['label'] == 'pos']\n",
    "df = pd.concat((df_pos[:DATA_LIMIT], df_neg[:DATA_LIMIT]))\n",
    "df_test=pd.concat((df_pos[1000:1500], df_neg[1000:1500]))\n",
    "def process_text(input_string, return_string=False, stem=False):\n",
    "    text = nlp(u'' + input_string)\n",
    "    if stem == True:\n",
    "        text = [tok.lemma_ for tok in text if (tok.is_alpha and not tok.is_stop)]\n",
    "    else:\n",
    "        text = [tok.lower_ for tok in text if (tok.is_alpha and not tok.is_stop)]\n",
    "    if return_string == True:\n",
    "        return \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2000/2000 [01:30<00:00, 22.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# Make this statement true to run from scratch [It takes time to process the text]\n",
    "if 1 != 0:\n",
    "    wordlist = []\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        wordlist.append(process_text(df['review'].iloc[i]))\n",
    "        \n",
    "    with open('vocabulary.txt', 'wb') as vocabulary:\n",
    "        pickle.dump(wordlist, vocabulary)\n",
    "    vocabulary.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load vocabulary\n",
    "wordlist = []\n",
    "with open('vocabulary.txt', 'rb') as vocabulary:\n",
    "    wordlist = pickle.load(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens - 1018\n"
     ]
    }
   ],
   "source": [
    "# Keeping track of frequency of a single token\n",
    "frequency = defaultdict(int)\n",
    "for text in wordlist:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "        \n",
    "# Apply Threshold to limit the vocabulary size, discarding the tokens which appeard number of times below the threshold limit \n",
    "FREQ_THRESHOLD = 35\n",
    "\n",
    "thresholded_wordlist =  [[token for token in text if frequency[token] > FREQ_THRESHOLD]\n",
    "          for text in wordlist]\n",
    "\n",
    "# Create Dictionary based on the word list\n",
    "dictionary = Dictionary(thresholded_wordlist)\n",
    "\n",
    "# Number of tokens\n",
    "print(\"Number of Tokens - {}\".format(len(dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 2000/2000 [00:00<00:00, 4423.83it/s]\n"
     ]
    }
   ],
   "source": [
    "dump =\"\"\n",
    "for i in tqdm(range(2000)):\n",
    "    dump += df['review'].iloc[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('dump_fasttext', 'w', encoding ='utf-8') as f:\n",
    "    for i in range(2000):\n",
    "        f.write(df['review'].iloc[i].lower())\n",
    "        \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12500</th>\n",
       "      <td>12500</td>\n",
       "      <td>test</td>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0_10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12501</th>\n",
       "      <td>12501</td>\n",
       "      <td>test</td>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>pos</td>\n",
       "      <td>10000_7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12502</th>\n",
       "      <td>12502</td>\n",
       "      <td>test</td>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>pos</td>\n",
       "      <td>10001_9.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12503</th>\n",
       "      <td>12503</td>\n",
       "      <td>test</td>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>10002_8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12504</th>\n",
       "      <td>12504</td>\n",
       "      <td>test</td>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>pos</td>\n",
       "      <td>10003_8.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  type                                             review  \\\n",
       "12500       12500  test  I went and saw this movie last night after bei...   \n",
       "12501       12501  test  Actor turned director Bill Paxton follows up h...   \n",
       "12502       12502  test  As a recreational golfer with some knowledge o...   \n",
       "12503       12503  test  I saw this film in a sneak preview, and it is ...   \n",
       "12504       12504  test  Bill Paxton has taken the true story of the 19...   \n",
       "\n",
       "      label         file  \n",
       "12500   pos     0_10.txt  \n",
       "12501   pos  10000_7.txt  \n",
       "12502   pos  10001_9.txt  \n",
       "12503   pos  10002_8.txt  \n",
       "12504   pos  10003_8.txt  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spacy_obj = nlp(u''+dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#spacy_doc = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1018"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx = { dictionary[idx]:idx for idx in range(len(dictionary))}\n",
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word2idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-7755d13235a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mword2idx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'word2idx' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "525604"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spacy_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok5 = spacy_obj[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie\n"
     ]
    }
   ],
   "source": [
    "print(tok5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "last"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok5.nbor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "this"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok5.nbor(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1018, 1018)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot = np.zeros((1018,1018))\n",
    "one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "one_hot = np.eye(1018)\n",
    "print(one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getIndex(word):\n",
    "    try:\n",
    "        idx = word2idx[word]\n",
    "    except KeyError:\n",
    "        idx = -1\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_generation(word):\n",
    "    ind1 = np.zeros((4))\n",
    "    ind1[0] = getIndex(str(word.nbor(-2)))\n",
    "    ind1[1] = getIndex(str(word.nbor(-1)))\n",
    "    ind1[2] = getIndex(str(word.nbor(1)))\n",
    "    ind1[3] = getIndex(str(word.nbor(2)))\n",
    "    return ind1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1018"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 525602/525602 [00:35<00:00, 14864.37it/s]\n"
     ]
    }
   ],
   "source": [
    "len_s = len(spacy_obj)\n",
    "inp = np.zeros((len_s,4))\n",
    "target = np.zeros((len_s))\n",
    "for i in tqdm(range(len_s-2)):\n",
    "    token = spacy_obj[i]\n",
    "    token1 = str(token)\n",
    "    index = getIndex(token1)\n",
    "    target[i]=index\n",
    "    inp[i,:] = input_generation(spacy_obj[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525604, 4)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_pos = target[np.where(target!=-1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = inp[np.where(target!=-1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.0"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_pos)\n",
    "target_pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=1019, output_dim=50, input_length = 4))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(50,)))\n",
    "cbow.add(Dense(1019, activation = 'softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cbow.compile(loss='sparse_categorical_crossentropy', optimizer ='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = inp+1\n",
    "train_y = target_pos+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108024, 4)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108024,)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1018.0"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "108024/108024 [==============================] - ETA: 33:28 - loss: 6.92 - ETA: 17:26 - loss: 6.92 - ETA: 11:56 - loss: 6.92 - ETA: 6:06 - loss: 6.9263 - ETA: 4:09 - loss: 6.925 - ETA: 3:10 - loss: 6.925 - ETA: 2:35 - loss: 6.925 - ETA: 2:11 - loss: 6.924 - ETA: 1:54 - loss: 6.924 - ETA: 1:41 - loss: 6.924 - ETA: 1:32 - loss: 6.923 - ETA: 1:24 - loss: 6.923 - ETA: 1:17 - loss: 6.923 - ETA: 1:12 - loss: 6.923 - ETA: 1:07 - loss: 6.922 - ETA: 1:03 - loss: 6.922 - ETA: 1:03 - loss: 6.922 - ETA: 59s - loss: 6.922 - ETA: 56s - loss: 6.92 - ETA: 54s - loss: 6.92 - ETA: 52s - loss: 6.92 - ETA: 49s - loss: 6.92 - ETA: 48s - loss: 6.92 - ETA: 46s - loss: 6.92 - ETA: 44s - loss: 6.92 - ETA: 43s - loss: 6.91 - ETA: 42s - loss: 6.91 - ETA: 40s - loss: 6.91 - ETA: 39s - loss: 6.91 - ETA: 38s - loss: 6.91 - ETA: 37s - loss: 6.91 - ETA: 36s - loss: 6.91 - ETA: 35s - loss: 6.91 - ETA: 34s - loss: 6.91 - ETA: 34s - loss: 6.91 - ETA: 33s - loss: 6.91 - ETA: 32s - loss: 6.91 - ETA: 32s - loss: 6.91 - ETA: 31s - loss: 6.91 - ETA: 30s - loss: 6.91 - ETA: 30s - loss: 6.91 - ETA: 29s - loss: 6.91 - ETA: 29s - loss: 6.91 - ETA: 28s - loss: 6.91 - ETA: 28s - loss: 6.91 - ETA: 27s - loss: 6.91 - ETA: 27s - loss: 6.91 - ETA: 27s - loss: 6.91 - ETA: 26s - loss: 6.91 - ETA: 26s - loss: 6.91 - ETA: 25s - loss: 6.91 - ETA: 25s - loss: 6.90 - ETA: 25s - loss: 6.90 - ETA: 24s - loss: 6.90 - ETA: 24s - loss: 6.90 - ETA: 24s - loss: 6.90 - ETA: 23s - loss: 6.90 - ETA: 23s - loss: 6.90 - ETA: 23s - loss: 6.90 - ETA: 23s - loss: 6.90 - ETA: 22s - loss: 6.90 - ETA: 22s - loss: 6.90 - ETA: 22s - loss: 6.90 - ETA: 21s - loss: 6.90 - ETA: 21s - loss: 6.90 - ETA: 21s - loss: 6.90 - ETA: 21s - loss: 6.90 - ETA: 21s - loss: 6.89 - ETA: 20s - loss: 6.89 - ETA: 20s - loss: 6.89 - ETA: 20s - loss: 6.89 - ETA: 20s - loss: 6.89 - ETA: 19s - loss: 6.89 - ETA: 19s - loss: 6.89 - ETA: 19s - loss: 6.89 - ETA: 19s - loss: 6.89 - ETA: 19s - loss: 6.89 - ETA: 18s - loss: 6.88 - ETA: 18s - loss: 6.88 - ETA: 18s - loss: 6.88 - ETA: 18s - loss: 6.88 - ETA: 18s - loss: 6.88 - ETA: 18s - loss: 6.88 - ETA: 17s - loss: 6.88 - ETA: 17s - loss: 6.88 - ETA: 17s - loss: 6.87 - ETA: 17s - loss: 6.87 - ETA: 17s - loss: 6.87 - ETA: 17s - loss: 6.87 - ETA: 16s - loss: 6.87 - ETA: 16s - loss: 6.87 - ETA: 16s - loss: 6.87 - ETA: 16s - loss: 6.86 - ETA: 16s - loss: 6.86 - ETA: 16s - loss: 6.86 - ETA: 16s - loss: 6.86 - ETA: 16s - loss: 6.86 - ETA: 15s - loss: 6.86 - ETA: 15s - loss: 6.86 - ETA: 15s - loss: 6.85 - ETA: 15s - loss: 6.85 - ETA: 15s - loss: 6.85 - ETA: 15s - loss: 6.85 - ETA: 15s - loss: 6.85 - ETA: 14s - loss: 6.85 - ETA: 14s - loss: 6.84 - ETA: 14s - loss: 6.84 - ETA: 14s - loss: 6.84 - ETA: 14s - loss: 6.84 - ETA: 14s - loss: 6.84 - ETA: 14s - loss: 6.83 - ETA: 14s - loss: 6.83 - ETA: 14s - loss: 6.83 - ETA: 13s - loss: 6.83 - ETA: 13s - loss: 6.83 - ETA: 13s - loss: 6.82 - ETA: 13s - loss: 6.82 - ETA: 13s - loss: 6.82 - ETA: 13s - loss: 6.82 - ETA: 13s - loss: 6.82 - ETA: 13s - loss: 6.81 - ETA: 13s - loss: 6.81 - ETA: 12s - loss: 6.81 - ETA: 12s - loss: 6.81 - ETA: 12s - loss: 6.81 - ETA: 12s - loss: 6.81 - ETA: 12s - loss: 6.80 - ETA: 12s - loss: 6.80 - ETA: 12s - loss: 6.80 - ETA: 12s - loss: 6.80 - ETA: 12s - loss: 6.80 - ETA: 12s - loss: 6.80 - ETA: 12s - loss: 6.79 - ETA: 11s - loss: 6.79 - ETA: 11s - loss: 6.79 - ETA: 11s - loss: 6.79 - ETA: 11s - loss: 6.79 - ETA: 11s - loss: 6.79 - ETA: 11s - loss: 6.78 - ETA: 11s - loss: 6.78 - ETA: 11s - loss: 6.78 - ETA: 11s - loss: 6.78 - ETA: 11s - loss: 6.78 - ETA: 10s - loss: 6.77 - ETA: 10s - loss: 6.77 - ETA: 10s - loss: 6.77 - ETA: 10s - loss: 6.77 - ETA: 10s - loss: 6.77 - ETA: 10s - loss: 6.77 - ETA: 10s - loss: 6.76 - ETA: 10s - loss: 6.76 - ETA: 10s - loss: 6.76 - ETA: 10s - loss: 6.76 - ETA: 9s - loss: 6.7636 - ETA: 9s - loss: 6.762 - ETA: 9s - loss: 6.761 - ETA: 9s - loss: 6.760 - ETA: 9s - loss: 6.759 - ETA: 9s - loss: 6.758 - ETA: 9s - loss: 6.756 - ETA: 9s - loss: 6.755 - ETA: 9s - loss: 6.754 - ETA: 9s - loss: 6.752 - ETA: 9s - loss: 6.751 - ETA: 9s - loss: 6.750 - ETA: 8s - loss: 6.748 - ETA: 8s - loss: 6.747 - ETA: 8s - loss: 6.745 - ETA: 8s - loss: 6.743 - ETA: 8s - loss: 6.742 - ETA: 8s - loss: 6.741 - ETA: 8s - loss: 6.740 - ETA: 8s - loss: 6.738 - ETA: 8s - loss: 6.736 - ETA: 8s - loss: 6.735 - ETA: 8s - loss: 6.734 - ETA: 7s - loss: 6.733 - ETA: 7s - loss: 6.732 - ETA: 7s - loss: 6.731 - ETA: 7s - loss: 6.730 - ETA: 7s - loss: 6.729 - ETA: 7s - loss: 6.727 - ETA: 7s - loss: 6.726 - ETA: 7s - loss: 6.725 - ETA: 7s - loss: 6.724 - ETA: 7s - loss: 6.723 - ETA: 7s - loss: 6.721 - ETA: 7s - loss: 6.721 - ETA: 7s - loss: 6.719 - ETA: 6s - loss: 6.718 - ETA: 6s - loss: 6.717 - ETA: 6s - loss: 6.716 - ETA: 6s - loss: 6.716 - ETA: 6s - loss: 6.715 - ETA: 6s - loss: 6.713 - ETA: 6s - loss: 6.712 - ETA: 6s - loss: 6.711 - ETA: 6s - loss: 6.710 - ETA: 6s - loss: 6.709 - ETA: 6s - loss: 6.708 - ETA: 6s - loss: 6.707 - ETA: 6s - loss: 6.706 - ETA: 5s - loss: 6.705 - ETA: 5s - loss: 6.704 - ETA: 5s - loss: 6.702 - ETA: 5s - loss: 6.701 - ETA: 5s - loss: 6.700 - ETA: 5s - loss: 6.699 - ETA: 5s - loss: 6.698 - ETA: 5s - loss: 6.697 - ETA: 5s - loss: 6.696 - ETA: 5s - loss: 6.695 - ETA: 5s - loss: 6.694 - ETA: 5s - loss: 6.694 - ETA: 5s - loss: 6.693 - ETA: 5s - loss: 6.692 - ETA: 4s - loss: 6.691 - ETA: 4s - loss: 6.690 - ETA: 4s - loss: 6.690 - ETA: 4s - loss: 6.689 - ETA: 4s - loss: 6.688 - ETA: 4s - loss: 6.687 - ETA: 4s - loss: 6.686 - ETA: 4s - loss: 6.685 - ETA: 4s - loss: 6.685 - ETA: 4s - loss: 6.684 - ETA: 4s - loss: 6.683 - ETA: 4s - loss: 6.682 - ETA: 4s - loss: 6.681 - ETA: 4s - loss: 6.680 - ETA: 3s - loss: 6.680 - ETA: 3s - loss: 6.679 - ETA: 3s - loss: 6.678 - ETA: 3s - loss: 6.677 - ETA: 3s - loss: 6.677 - ETA: 3s - loss: 6.675 - ETA: 3s - loss: 6.674 - ETA: 3s - loss: 6.674 - ETA: 3s - loss: 6.673 - ETA: 3s - loss: 6.672 - ETA: 3s - loss: 6.671 - ETA: 3s - loss: 6.671 - ETA: 3s - loss: 6.670 - ETA: 3s - loss: 6.669 - ETA: 2s - loss: 6.668 - ETA: 2s - loss: 6.667 - ETA: 2s - loss: 6.667 - ETA: 2s - loss: 6.666 - ETA: 2s - loss: 6.665 - ETA: 2s - loss: 6.665 - ETA: 2s - loss: 6.664 - ETA: 2s - loss: 6.664 - ETA: 2s - loss: 6.663 - ETA: 2s - loss: 6.663 - ETA: 2s - loss: 6.662 - ETA: 2s - loss: 6.661 - ETA: 2s - loss: 6.660 - ETA: 2s - loss: 6.659 - ETA: 1s - loss: 6.659 - ETA: 1s - loss: 6.658 - ETA: 1s - loss: 6.658 - ETA: 1s - loss: 6.657 - ETA: 1s - loss: 6.656 - ETA: 1s - loss: 6.655 - ETA: 1s - loss: 6.654 - ETA: 1s - loss: 6.653 - ETA: 1s - loss: 6.653 - ETA: 1s - loss: 6.652 - ETA: 1s - loss: 6.651 - ETA: 1s - loss: 6.651 - ETA: 1s - loss: 6.650 - ETA: 1s - loss: 6.649 - ETA: 1s - loss: 6.648 - ETA: 0s - loss: 6.647 - ETA: 0s - loss: 6.647 - ETA: 0s - loss: 6.646 - ETA: 0s - loss: 6.645 - ETA: 0s - loss: 6.644 - ETA: 0s - loss: 6.643 - ETA: 0s - loss: 6.643 - ETA: 0s - loss: 6.642 - ETA: 0s - loss: 6.642 - ETA: 0s - loss: 6.642 - ETA: 0s - loss: 6.641 - ETA: 0s - loss: 6.641 - ETA: 0s - loss: 6.640 - ETA: 0s - loss: 6.640 - 19s 177us/step - loss: 6.6398\n",
      "Epoch 2/5\n",
      "108024/108024 [==============================] - ETA: 16s - loss: 6.55 - ETA: 15s - loss: 6.54 - ETA: 15s - loss: 6.51 - ETA: 15s - loss: 6.51 - ETA: 15s - loss: 6.50 - ETA: 15s - loss: 6.49 - ETA: 15s - loss: 6.48 - ETA: 15s - loss: 6.48 - ETA: 15s - loss: 6.47 - ETA: 15s - loss: 6.47 - ETA: 15s - loss: 6.46 - ETA: 15s - loss: 6.47 - ETA: 15s - loss: 6.47 - ETA: 15s - loss: 6.48 - ETA: 15s - loss: 6.47 - ETA: 15s - loss: 6.47 - ETA: 14s - loss: 6.47 - ETA: 14s - loss: 6.47 - ETA: 14s - loss: 6.47 - ETA: 14s - loss: 6.47 - ETA: 14s - loss: 6.47 - ETA: 14s - loss: 6.47 - ETA: 14s - loss: 6.48 - ETA: 14s - loss: 6.48 - ETA: 14s - loss: 6.47 - ETA: 14s - loss: 6.47 - ETA: 14s - loss: 6.47 - ETA: 14s - loss: 6.47 - ETA: 14s - loss: 6.47 - ETA: 14s - loss: 6.47 - ETA: 14s - loss: 6.47 - ETA: 14s - loss: 6.47 - ETA: 14s - loss: 6.47 - ETA: 13s - loss: 6.47 - ETA: 13s - loss: 6.47 - ETA: 13s - loss: 6.48 - ETA: 13s - loss: 6.48 - ETA: 13s - loss: 6.48 - ETA: 13s - loss: 6.47 - ETA: 13s - loss: 6.47 - ETA: 13s - loss: 6.47 - ETA: 13s - loss: 6.47 - ETA: 13s - loss: 6.47 - ETA: 13s - loss: 6.47 - ETA: 13s - loss: 6.48 - ETA: 13s - loss: 6.48 - ETA: 13s - loss: 6.48 - ETA: 13s - loss: 6.48 - ETA: 13s - loss: 6.48 - ETA: 13s - loss: 6.48 - ETA: 13s - loss: 6.47 - ETA: 13s - loss: 6.47 - ETA: 13s - loss: 6.48 - ETA: 13s - loss: 6.48 - ETA: 13s - loss: 6.48 - ETA: 13s - loss: 6.48 - ETA: 12s - loss: 6.48 - ETA: 12s - loss: 6.48 - ETA: 12s - loss: 6.47 - ETA: 12s - loss: 6.47 - ETA: 12s - loss: 6.47 - ETA: 12s - loss: 6.47 - ETA: 12s - loss: 6.47 - ETA: 12s - loss: 6.47 - ETA: 12s - loss: 6.47 - ETA: 12s - loss: 6.47 - ETA: 12s - loss: 6.47 - ETA: 12s - loss: 6.47 - ETA: 12s - loss: 6.47 - ETA: 12s - loss: 6.47 - ETA: 12s - loss: 6.47 - ETA: 12s - loss: 6.47 - ETA: 12s - loss: 6.47 - ETA: 12s - loss: 6.47 - ETA: 11s - loss: 6.47 - ETA: 11s - loss: 6.47 - ETA: 11s - loss: 6.47 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 10s - loss: 6.46 - ETA: 10s - loss: 6.46 - ETA: 10s - loss: 6.46 - ETA: 10s - loss: 6.46 - ETA: 10s - loss: 6.46 - ETA: 10s - loss: 6.46 - ETA: 10s - loss: 6.46 - ETA: 10s - loss: 6.46 - ETA: 10s - loss: 6.46 - ETA: 10s - loss: 6.46 - ETA: 10s - loss: 6.46 - ETA: 10s - loss: 6.46 - ETA: 10s - loss: 6.46 - ETA: 10s - loss: 6.46 - ETA: 10s - loss: 6.46 - ETA: 10s - loss: 6.46 - ETA: 10s - loss: 6.46 - ETA: 10s - loss: 6.46 - ETA: 9s - loss: 6.4698 - ETA: 9s - loss: 6.471 - ETA: 9s - loss: 6.472 - ETA: 9s - loss: 6.472 - ETA: 9s - loss: 6.471 - ETA: 9s - loss: 6.470 - ETA: 9s - loss: 6.470 - ETA: 9s - loss: 6.470 - ETA: 9s - loss: 6.470 - ETA: 9s - loss: 6.469 - ETA: 9s - loss: 6.470 - ETA: 9s - loss: 6.471 - ETA: 9s - loss: 6.471 - ETA: 9s - loss: 6.469 - ETA: 9s - loss: 6.469 - ETA: 9s - loss: 6.469 - ETA: 9s - loss: 6.469 - ETA: 9s - loss: 6.469 - ETA: 8s - loss: 6.469 - ETA: 8s - loss: 6.468 - ETA: 8s - loss: 6.468 - ETA: 8s - loss: 6.468 - ETA: 8s - loss: 6.468 - ETA: 8s - loss: 6.468 - ETA: 8s - loss: 6.468 - ETA: 8s - loss: 6.467 - ETA: 8s - loss: 6.467 - ETA: 8s - loss: 6.467 - ETA: 8s - loss: 6.467 - ETA: 8s - loss: 6.467 - ETA: 8s - loss: 6.467 - ETA: 8s - loss: 6.466 - ETA: 8s - loss: 6.466 - ETA: 8s - loss: 6.466 - ETA: 8s - loss: 6.466 - ETA: 7s - loss: 6.466 - ETA: 7s - loss: 6.465 - ETA: 7s - loss: 6.466 - ETA: 7s - loss: 6.465 - ETA: 7s - loss: 6.466 - ETA: 7s - loss: 6.466 - ETA: 7s - loss: 6.466 - ETA: 7s - loss: 6.466 - ETA: 7s - loss: 6.467 - ETA: 7s - loss: 6.467 - ETA: 7s - loss: 6.467 - ETA: 7s - loss: 6.467 - ETA: 7s - loss: 6.467 - ETA: 7s - loss: 6.467 - ETA: 7s - loss: 6.466 - ETA: 7s - loss: 6.466 - ETA: 7s - loss: 6.466 - ETA: 7s - loss: 6.465 - ETA: 6s - loss: 6.465 - ETA: 6s - loss: 6.465 - ETA: 6s - loss: 6.465 - ETA: 6s - loss: 6.464 - ETA: 6s - loss: 6.464 - ETA: 6s - loss: 6.464 - ETA: 6s - loss: 6.464 - ETA: 6s - loss: 6.464 - ETA: 6s - loss: 6.463 - ETA: 6s - loss: 6.464 - ETA: 6s - loss: 6.463 - ETA: 6s - loss: 6.463 - ETA: 6s - loss: 6.462 - ETA: 6s - loss: 6.462 - ETA: 6s - loss: 6.463 - ETA: 6s - loss: 6.462 - ETA: 6s - loss: 6.462 - ETA: 6s - loss: 6.462 - ETA: 5s - loss: 6.462 - ETA: 5s - loss: 6.461 - ETA: 5s - loss: 6.462 - ETA: 5s - loss: 6.462 - ETA: 5s - loss: 6.461 - ETA: 5s - loss: 6.462 - ETA: 5s - loss: 6.462 - ETA: 5s - loss: 6.462 - ETA: 5s - loss: 6.462 - ETA: 5s - loss: 6.463 - ETA: 5s - loss: 6.463 - ETA: 5s - loss: 6.463 - ETA: 5s - loss: 6.463 - ETA: 5s - loss: 6.463 - ETA: 5s - loss: 6.463 - ETA: 5s - loss: 6.463 - ETA: 5s - loss: 6.463 - ETA: 4s - loss: 6.463 - ETA: 4s - loss: 6.463 - ETA: 4s - loss: 6.464 - ETA: 4s - loss: 6.463 - ETA: 4s - loss: 6.464 - ETA: 4s - loss: 6.464 - ETA: 4s - loss: 6.464 - ETA: 4s - loss: 6.465 - ETA: 4s - loss: 6.465 - ETA: 4s - loss: 6.465 - ETA: 4s - loss: 6.465 - ETA: 4s - loss: 6.465 - ETA: 4s - loss: 6.465 - ETA: 4s - loss: 6.466 - ETA: 4s - loss: 6.466 - ETA: 4s - loss: 6.466 - ETA: 4s - loss: 6.465 - ETA: 3s - loss: 6.466 - ETA: 3s - loss: 6.466 - ETA: 3s - loss: 6.465 - ETA: 3s - loss: 6.465 - ETA: 3s - loss: 6.465 - ETA: 3s - loss: 6.465 - ETA: 3s - loss: 6.465 - ETA: 3s - loss: 6.465 - ETA: 3s - loss: 6.465 - ETA: 3s - loss: 6.464 - ETA: 3s - loss: 6.465 - ETA: 3s - loss: 6.465 - ETA: 3s - loss: 6.465 - ETA: 3s - loss: 6.465 - ETA: 3s - loss: 6.465 - ETA: 3s - loss: 6.465 - ETA: 3s - loss: 6.465 - ETA: 3s - loss: 6.465 - ETA: 3s - loss: 6.465 - ETA: 2s - loss: 6.465 - ETA: 2s - loss: 6.465 - ETA: 2s - loss: 6.464 - ETA: 2s - loss: 6.464 - ETA: 2s - loss: 6.464 - ETA: 2s - loss: 6.464 - ETA: 2s - loss: 6.464 - ETA: 2s - loss: 6.464 - ETA: 2s - loss: 6.464 - ETA: 2s - loss: 6.464 - ETA: 2s - loss: 6.464 - ETA: 2s - loss: 6.465 - ETA: 2s - loss: 6.465 - ETA: 2s - loss: 6.465 - ETA: 2s - loss: 6.465 - ETA: 2s - loss: 6.464 - ETA: 2s - loss: 6.464 - ETA: 1s - loss: 6.464 - ETA: 1s - loss: 6.464 - ETA: 1s - loss: 6.465 - ETA: 1s - loss: 6.465 - ETA: 1s - loss: 6.465 - ETA: 1s - loss: 6.464 - ETA: 1s - loss: 6.464 - ETA: 1s - loss: 6.465 - ETA: 1s - loss: 6.464 - ETA: 1s - loss: 6.464 - ETA: 1s - loss: 6.464 - ETA: 1s - loss: 6.464 - ETA: 1s - loss: 6.464 - ETA: 1s - loss: 6.465 - ETA: 1s - loss: 6.465 - ETA: 1s - loss: 6.465 - ETA: 1s - loss: 6.464 - ETA: 0s - loss: 6.465 - ETA: 0s - loss: 6.465 - ETA: 0s - loss: 6.465 - ETA: 0s - loss: 6.465 - ETA: 0s - loss: 6.465 - ETA: 0s - loss: 6.465 - ETA: 0s - loss: 6.465 - ETA: 0s - loss: 6.465 - ETA: 0s - loss: 6.465 - ETA: 0s - loss: 6.465 - ETA: 0s - loss: 6.466 - ETA: 0s - loss: 6.466 - ETA: 0s - loss: 6.466 - ETA: 0s - loss: 6.466 - ETA: 0s - loss: 6.466 - ETA: 0s - loss: 6.466 - ETA: 0s - loss: 6.466 - 16s 152us/step - loss: 6.4664\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 53760/108024 [=============>................] - ETA: 20s - loss: 6.47 - ETA: 15s - loss: 6.43 - ETA: 16s - loss: 6.47 - ETA: 16s - loss: 6.47 - ETA: 15s - loss: 6.46 - ETA: 15s - loss: 6.46 - ETA: 15s - loss: 6.46 - ETA: 15s - loss: 6.46 - ETA: 15s - loss: 6.47 - ETA: 15s - loss: 6.48 - ETA: 15s - loss: 6.48 - ETA: 15s - loss: 6.47 - ETA: 15s - loss: 6.47 - ETA: 15s - loss: 6.47 - ETA: 15s - loss: 6.47 - ETA: 15s - loss: 6.46 - ETA: 15s - loss: 6.46 - ETA: 15s - loss: 6.46 - ETA: 14s - loss: 6.46 - ETA: 14s - loss: 6.46 - ETA: 14s - loss: 6.46 - ETA: 14s - loss: 6.46 - ETA: 14s - loss: 6.46 - ETA: 14s - loss: 6.46 - ETA: 14s - loss: 6.46 - ETA: 14s - loss: 6.45 - ETA: 14s - loss: 6.46 - ETA: 14s - loss: 6.46 - ETA: 14s - loss: 6.46 - ETA: 14s - loss: 6.45 - ETA: 14s - loss: 6.46 - ETA: 14s - loss: 6.45 - ETA: 14s - loss: 6.46 - ETA: 14s - loss: 6.46 - ETA: 14s - loss: 6.46 - ETA: 14s - loss: 6.46 - ETA: 14s - loss: 6.46 - ETA: 14s - loss: 6.46 - ETA: 13s - loss: 6.45 - ETA: 14s - loss: 6.45 - ETA: 13s - loss: 6.45 - ETA: 13s - loss: 6.45 - ETA: 13s - loss: 6.45 - ETA: 13s - loss: 6.45 - ETA: 13s - loss: 6.45 - ETA: 13s - loss: 6.45 - ETA: 13s - loss: 6.45 - ETA: 13s - loss: 6.45 - ETA: 13s - loss: 6.45 - ETA: 13s - loss: 6.45 - ETA: 13s - loss: 6.45 - ETA: 13s - loss: 6.45 - ETA: 13s - loss: 6.45 - ETA: 13s - loss: 6.45 - ETA: 13s - loss: 6.45 - ETA: 13s - loss: 6.45 - ETA: 13s - loss: 6.45 - ETA: 13s - loss: 6.45 - ETA: 13s - loss: 6.45 - ETA: 12s - loss: 6.45 - ETA: 12s - loss: 6.45 - ETA: 12s - loss: 6.45 - ETA: 12s - loss: 6.45 - ETA: 12s - loss: 6.45 - ETA: 12s - loss: 6.45 - ETA: 12s - loss: 6.45 - ETA: 12s - loss: 6.45 - ETA: 12s - loss: 6.45 - ETA: 12s - loss: 6.46 - ETA: 12s - loss: 6.45 - ETA: 12s - loss: 6.46 - ETA: 12s - loss: 6.46 - ETA: 12s - loss: 6.46 - ETA: 12s - loss: 6.46 - ETA: 12s - loss: 6.46 - ETA: 12s - loss: 6.46 - ETA: 12s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.45 - ETA: 11s - loss: 6.45 - ETA: 11s - loss: 6.45 - ETA: 11s - loss: 6.46 - ETA: 11s - loss: 6.45 - ETA: 11s - loss: 6.45 - ETA: 10s - loss: 6.45 - ETA: 10s - loss: 6.45 - ETA: 10s - loss: 6.45 - ETA: 10s - loss: 6.45 - ETA: 10s - loss: 6.45 - ETA: 10s - loss: 6.45 - ETA: 10s - loss: 6.45 - ETA: 10s - loss: 6.45 - ETA: 10s - loss: 6.46 - ETA: 10s - loss: 6.45 - ETA: 10s - loss: 6.45 - ETA: 10s - loss: 6.45 - ETA: 10s - loss: 6.45 - ETA: 10s - loss: 6.45 - ETA: 10s - loss: 6.45 - ETA: 10s - loss: 6.45 - ETA: 9s - loss: 6.4580 - ETA: 9s - loss: 6.457 - ETA: 9s - loss: 6.457 - ETA: 9s - loss: 6.457 - ETA: 9s - loss: 6.458 - ETA: 9s - loss: 6.458 - ETA: 9s - loss: 6.458 - ETA: 9s - loss: 6.458 - ETA: 9s - loss: 6.458 - ETA: 9s - loss: 6.458 - ETA: 9s - loss: 6.457 - ETA: 9s - loss: 6.458 - ETA: 9s - loss: 6.457 - ETA: 9s - loss: 6.457 - ETA: 9s - loss: 6.458 - ETA: 9s - loss: 6.458 - ETA: 9s - loss: 6.458 - ETA: 8s - loss: 6.458 - ETA: 8s - loss: 6.459 - ETA: 8s - loss: 6.458 - ETA: 8s - loss: 6.458 - ETA: 8s - loss: 6.458 - ETA: 8s - loss: 6.458 - ETA: 8s - loss: 6.459 - ETA: 8s - loss: 6.459 - ETA: 8s - loss: 6.459 - ETA: 8s - loss: 6.460 - ETA: 8s - loss: 6.459 - ETA: 8s - loss: 6.460 - ETA: 8s - loss: 6.4590"
     ]
    }
   ],
   "source": [
    "cbow.fit(train_x, train_y,batch_size =128,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    token = spacy_obj[i]\n",
    "    if(i==0):\n",
    "        inp[i,0]=-1\n",
    "        inp[i,1]=-1\n",
    "        inp[i,2]=i+1\n",
    "        inp[i,3]=i+2\n",
    "    else if(i==1):\n",
    "        inp[i,0]=-1\n",
    "        inp[i,1]=i-1\n",
    "        inp[i,2]=i+2\n",
    "        inp[i,3]=i+3\n",
    "        \n",
    "    else if(i==)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(5464 unique tokens: ['able', 'admit', 'ben', 'character', 'comedy']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://i.imgur.com/f1uzTDZ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* From the screenshot you can see the implementation of word-cooccurance matrix, based on the tokens from the dictionary, build a word-cooccurance matrix yourself which is $X$. Documentation of gensim [https://radimrehurek.com/gensim/corpora/dictionary.html]\n",
    "* Apply SVD on $X$\n",
    "* Reduce Dimension \n",
    "\n",
    "![dimen_reduc](https://i.imgur.com/lezB870.png)\n",
    "\n",
    "* Here Richard is taking only top two dimensions of the vector $U$, recommended size is *50* for now.\n",
    "\n",
    "![dimen_reduc_u](https://i.imgur.com/TA2Bmsq.png)\n",
    "\n",
    "* Now we can get a fixed size vector for each word. \n",
    "\n",
    "* Try to plot something similar based on the given dataset. In class we will try to implement a logistic regression classifier that can classify positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_d=len(dictionary)\n",
    "X=np.zeros((len_d,len_d))\n",
    "la=np.linalg\n",
    "for sen in thresholded_wordlist:\n",
    "    idx=dictionary.doc2idx(sen)\n",
    "    for i in idx:\n",
    "        for j in idx:\n",
    "            X[i,j]=X[i,j]+1\n",
    "U,s,Vh = la.svd(X, full_matrices=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U_new=U[:,:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5464, 50)\n"
     ]
    }
   ],
   "source": [
    "print(U_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5464, 5464)\n"
     ]
    }
   ],
   "source": [
    "print(Vh.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAFexJREFUeJzt3X+QlXXd//HnG5hkJONX/gAR4R6Z\nWUFQdEVJKyx/QEb+iAZLa7FiJ28bp7vRuXX8Fv5q1L45Oo72NUZNxvyipjVRVg7+wNRBb5ZEAn8E\nIiW3ZhpgcKsp+f7+sYf97mdb3GXP2V0Wn4+ZM+e6rs/7us77ww68ONd1nbORmUiStF2/3m5AkrRr\nMRgkSQWDQZJUMBgkSQWDQZJUMBgkSQWDQZJUMBgkSQWDQZJUGNDbDXTFRz/60RwzZkxvtyFJfcry\n5ctfz8y9O6rrk8EwZswYmpqaersNSepTIuJPnanzVJIkqWAwSJIKBoMkqWAwSJIKBoMkqWAwSJIK\nBoMkqWAwSJIKBoMkqWAwSJIKBoMkqWAwSJIKBoMkqWAwSJIKBoMkqWAwSJIKBoMkqWAwSJIKBoMk\nqWAwSJIKBoMkqWAwSJIKBoMkqWAwSJIKNQmGiJgeEc9HxNqIuLCd8T0i4q7K+JMRMabN+OiI2BoR\n59eiH0lS11UdDBHRH7gRmAGMB74YEePblH0N2JSZBwHXAle3Gb8W+E21vUiSqleLdwxTgLWZuS4z\n3wHuBE5pU3MKsKCyfA/w6YgIgIg4FVgHrK5BL5KkKtUiGPYHXmq1vqGyrd2azNwGvAEMj4hBwH8C\nl9agD0lSDdQiGKKdbdnJmkuBazNza4cvEtEYEU0R0fTaa691oU1JUmcMqMExNgAHtFofBby8g5oN\nETEAGAxsBI4CZkXE94EhwHsR8XZm3tD2RTJzPjAfoL6+vm3wSJJqpBbBsAwYFxFjgf8GzgC+1KZm\nEdAALAVmAQ9lZgIf314QEZcAW9sLBUlSz6k6GDJzW0R8E7gf6A/cmpmrI+IyoCkzFwG3ALdHxFqa\n3ymcUe3rSpK6RzT/x71vqa+vz6ampt5uQ5L6lIhYnpn1HdX5yWdJUsFgkCQVDAZJUsFgkCQVDAZJ\nUsFgkCQVDAZJUsFgkCQVDAZJUsFgkCQVDAZJUsFgkCQVDAZJUsFgkCQVDAZJUsFgkCQVDAZJUsFg\nkCQVDAZJUsFgkCQVDAZJUsFgkCQVDAZJUsFgkCQVDAZJUsFgkCQVDAZJUsFgkCQVDAZJUsFgkCQV\nDAZJUqEmwRAR0yPi+YhYGxEXtjO+R0TcVRl/MiLGVLafEBHLI+IPledP1aIfSVLXVR0MEdEfuBGY\nAYwHvhgR49uUfQ3YlJkHAdcCV1e2vw7MzMyJQANwe7X9SJKqU4t3DFOAtZm5LjPfAe4ETmlTcwqw\noLJ8D/DpiIjMfCozX65sXw0MjIg9atCTJKmLahEM+wMvtVrfUNnWbk1mbgPeAIa3qfk88FRm/qMG\nPUmSumhADY4R7WzLnamJiAk0n146cYcvEtEINAKMHj1657uUJHVKLd4xbAAOaLU+Cnh5RzURMQAY\nDGysrI8Cfg58JTNf2NGLZOb8zKzPzPq99967Bm1LktpTi2BYBoyLiLER8SHgDGBRm5pFNF9cBpgF\nPJSZGRFDgPuAizLz8Rr0IkmqUtXBULlm8E3gfuBZ4O7MXB0Rl0XE5ypltwDDI2It8G1g+y2t3wQO\nAr4TESsqj32q7UmS1HWR2fZywK6vvr4+m5qaersNSepTImJ5ZtZ3VOcnnyVJBYNBklQwGCRJBYNB\nklQwGCRJBYNBklQwGCRJBYNBu7Vp06bhZ16knWMwSJIKBoN2C+vXr6euro6GhgYmTZrErFmzePPN\nN4uac845h/r6eiZMmMC8efMAePDBBznttNNaahYvXszpp5/eo71LuxqDQbuN559/nsbGRlauXMlH\nPvIRfvjDHxbj3/ve92hqamLlypU88sgjrFy5kk996lM8++yzvPbaawD8+Mc/5uyzz+6N9qVdhsGg\n3cYBBxzAMcccA8BZZ53FY489VozffffdHH744UyePJnVq1fzzDPPEBF8+ctf5ic/+QmbN29m6dKl\nzJgxozfal3YZtfhFPdIuISJ2uP7iiy/ygx/8gGXLljF06FDmzJnD22+/DcDZZ5/NzJkzGThwIF/4\nwhcYMMC/Fvpg8x2Ddht//vOfWbp0KQALFy7k2GOPbRn7+9//zqBBgxg8eDCvvvoqv/nNb1rGRo4c\nyciRI7niiiuYM2dOT7ct7XIMBu02Dj74YBYsWMCkSZPYuHEj55xzTsvYoYceyuTJk5kwYQJf/epX\nW045bXfmmWdywAEHMH78+J5uW9rl+J5Zu41+/fpx0003FduWLFnSsnzbbbftcN/HHnuMuXPndlNn\nUt9iMOgD74gjjmDQoEFcc801vd2KtEswGLRbGDNmDKtWrerSvsuXL69xN1Lf5jUGqQPr16/nkEMO\n6XT9nDlzuOeee/5l+5IlS/jsZz9by9akbmEwSJIKBoPUCf/85z+ZO3cuEyZM4MQTT+Stt95ixYoV\nHH300UyaNInTTjuNTZs2/ct+v/3tb6mrq+PYY4/lZz/7WS90Lu08g0HqhDVr1nDuueeyevVqhgwZ\nwr333stXvvIVrr76alauXMnEiRO59NJLi33efvtt5s6dyy9/+UseffRR/vKXv/RS99LOMRikThg7\ndiyHHXYY0HwX0wsvvMDmzZv55Cc/CUBDQwO/+93vin2ee+45xo4dy7hx44gIzjrrrB7vW+oKg0Hq\nhD322KNluX///mzevLlT+7X9mg6pLzAYpJ20ceNG7rjjDoYOHcqjjz4KwO23397y7mG7uro6Xnzx\nRV544QWg+Ws6Ouu6664rvjb8M5/5TKfDSKqWwSB10YIFC7jggguYNGkSK1as4Lvf/W4xPnDgQObP\nn8/JJ5/Msccey4EHHtjpY7cNhl//+tcMGTKkZr1L78dgkDrQ9sNzjY2NDB8+nBtvvJEtW7aw3377\nsXDhQu655x6OPPJInnrqKRYuXMibb77J9OnTOfroo/nWt77FVVddxa9+9Ss+/OEPA82fa5g2bRqz\nZs2irq6OM888k8zk+uuv5+WXX+a4447juOOOa+nh9ddfZ/369Rx88MH/cocUwLJly5g0aRJTp07l\nggsu2KnPXkitGQxSF7R3l9Lpp5/OsmXLePrppzn44IO55ZZbOjzOU089xXXXXcczzzzDunXrePzx\nxznvvPMYOXIkDz/8MA8//HCnXhuavz78pptuYunSpfTv37/mc9YHh8EgdUHbu5TWr1/PqlWr+PjH\nP87EiRO54447WL16dYfHmTJlCqNGjaJfv34cdthhrF+/vkuvvXnzZrZs2cLHPvYxAL70pS91fXL6\nwDMYpC5oe5fStm3bmDNnDjfccAN/+MMfmDdvXssvAhowYADvvfceAJnJO++8877H6cprZ2bVc5K2\nq0kwRMT0iHg+ItZGxIXtjO8REXdVxp+MiDGtxi6qbH8+Ik6qRT9ST7ntttv4+c9/DsCWLVsYMWIE\n7777LnfccUdLzZgxY1q+qO8Xv/gF7777Lps3b2br1q386U9/ave4e+21F1u2bOl0H0OHDmWvvfbi\niSeeAODOO+/s6pSk6oMhIvoDNwIzgPHAFyOi7W87+RqwKTMPAq4Frq7sOx44A5gATAd+WDme1Odc\nfvnlHHXUUZxwwgnU1dW1bJ87dy6PPPIIU6ZM4cknn2TQoEEMGTKErVu37vDUUWNjIzNmzGi5+NwZ\nt9xyC42NjUydOpXMZPDgwdVOSR9UmVnVA5gK3N9q/SLgojY19wNTK8sDgNeBaFvbuu79HkcccURK\nPeGUU07Jww8/PMePH58/+tGPMjPz1ltvzXHjxuUnPvGJ/PrXv57nnntuZmY2NDTkN77xjZw2bVqO\nHTs2lyxZkmeffXbW1dVlQ0NDyzEPPPDAfO2113L27Nk5cODAPPTQQ/P888+vutctW7a0LF955ZV5\n3nnnVX1M7V6ApuzEv+u1+H0M+wMvtVrfABy1o5rM3BYRbwDDK9ufaLPv/jXoSaqJW2+9lWHDhvHW\nW29x5JFHcvLJJzNv3jyWL1/O4MGDOe6445g8eXJL/aZNm3jooYdYtGgRM2fO5PHHH+fmm2/myCOP\nZMWKFS0XjQGuuuoqVq1axYoVK2rS63333ceVV17Jtm3bOPDAA9/3N9ZJ76cWwdDeZ/7bXgnbUU1n\n9m0+QEQj0AgwevTonelP6rLrr7++5RrCSy+9xO233860adPYe++9AZg9ezZ//OMfW+pnzpxJRDBx\n4kT23XdfJk6cCMCECRNYv359EQy1Nnv2bGbPnt1tx9cHRy0uPm8ADmi1Pgp4eUc1ETEAGAxs7OS+\nAGTm/Mysz8z67X8ppe60ZMkSHnjgAZYuXcrTTz/N5MmTqaure9/vP9p+x1C/fv2Ku4f69evXqTuO\npF1BLYJhGTAuIsZGxIdovpi8qE3NIqChsjwLeKhyvmsRcEblrqWxwDjgv2rQk1S1N954g6FDh7Ln\nnnvy3HPP8cQTT/DWW2+xZMkS/va3v/Huu+/y05/+tMvH39k7j6SeUnUwZOY24Js0Xzh+Frg7M1dH\nxGUR8blK2S3A8IhYC3wbuLCy72rgbuAZ4LfAuZn5z2p7kmph+vTpbNu2jUmTJvGd73yHo48+mhEj\nRnDJJZcwdepUjj/+eA4//PAuH3/48OEcc8wxHHLIIVxwwQU17FyqTmQf/GBMfX19NjU19XYbktSn\nRMTyzKzvqM5PPkuSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiS\nCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaD\nJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSClUFQ0QMi4jFEbGm8jx0B3UNlZo1EdFQ\n2bZnRNwXEc9FxOqIuKqaXiRJtVHtO4YLgQczcxzwYGW9EBHDgHnAUcAUYF6rAPlBZtYBk4FjImJG\nlf1IkqpUbTCcAiyoLC8ATm2n5iRgcWZuzMxNwGJgema+mZkPA2TmO8DvgVFV9iNJqlK1wbBvZr4C\nUHnep52a/YGXWq1vqGxrERFDgJk0v+uQJPWiAR0VRMQDwH7tDF3cydeIdrZlq+MPABYC12fmuvfp\noxFoBBg9enQnX1qStLM6DIbMPH5HYxHxakSMyMxXImIE8Nd2yjYA01qtjwKWtFqfD6zJzOs66GN+\npZb6+vp8v1pJUtdVeyppEdBQWW4AftFOzf3AiRExtHLR+cTKNiLiCmAw8K0q+5Ak1Ui1wXAVcEJE\nrAFOqKwTEfURcTNAZm4ELgeWVR6XZebGiBhF8+mo8cDvI2JFRHy9yn4kSVWKzL53Vqa+vj6bmpp6\nuw1J6lMiYnlm1ndU5yefJUkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GS\nVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAY\nJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVKgqGCJiWEQsjog1leehO6hrqNSsiYiG\ndsYXRcSqanqRJNVGte8YLgQezMxxwIOV9UJEDAPmAUcBU4B5rQMkIk4HtlbZhySpRqoNhlOABZXl\nBcCp7dScBCzOzI2ZuQlYDEwHiIgPA98GrqiyD0lSjVQbDPtm5isAled92qnZH3ip1fqGyjaAy4Fr\ngDer7EOSVCMDOiqIiAeA/doZuriTrxHtbMuIOAw4KDP/IyLGdKKPRqARYPTo0Z18aUnSzuowGDLz\n+B2NRcSrETEiM1+JiBHAX9sp2wBMa7U+ClgCTAWOiIj1lT72iYglmTmNdmTmfGA+QH19fXbUtySp\na6o9lbQI2H6XUQPwi3Zq7gdOjIihlYvOJwL3Z+b/ycyRmTkGOBb4445CQZLUc6oNhquAEyJiDXBC\nZZ2IqI+ImwEycyPN1xKWVR6XVbZJknZBkdn3zsrU19dnU1NTb7chSX1KRCzPzPqO6vzksySpYDBI\nkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoG\ngySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpEJnZ2z3stIh4\nDfhTb/exkz4KvN7bTfQw5/zB4Jz7jgMzc++OivpkMPRFEdGUmfW93UdPcs4fDM559+OpJElSwWCQ\nJBUMhp4zv7cb6AXO+YPBOe9mvMYgSSr4jkGSVDAYaigihkXE4ohYU3keuoO6hkrNmohoaGd8UUSs\n6v6Oq1fNnCNiz4i4LyKei4jVEXFVz3a/cyJiekQ8HxFrI+LCdsb3iIi7KuNPRsSYVmMXVbY/HxEn\n9WTf1ejqnCPihIhYHhF/qDx/qqd774pqfsaV8dERsTUizu+pnrtFZvqo0QP4PnBhZflC4Op2aoYB\n6yrPQyvLQ1uNnw78X2BVb8+nu+cM7AkcV6n5EPAoMKO357SDefYHXgD+rdLr08D4NjX/DtxUWT4D\nuKuyPL5SvwcwtnKc/r09p26e82RgZGX5EOC/e3s+3TnfVuP3Aj8Fzu/t+VTz8B1DbZ0CLKgsLwBO\nbafmJGBxZm7MzE3AYmA6QER8GPg2cEUP9ForXZ5zZr6ZmQ8DZOY7wO+BUT3Qc1dMAdZm5rpKr3fS\nPPfWWv9Z3AN8OiKisv3OzPxHZr4IrK0cb1fX5Tln5lOZ+XJl+2pgYETs0SNdd101P2Mi4lSa/9Oz\nuof67TYGQ23tm5mvAFSe92mnZn/gpVbrGyrbAC4HrgHe7M4ma6zaOQMQEUOAmcCD3dRntTqcQ+ua\nzNwGvAEM7+S+u6Jq5tza54GnMvMf3dRnrXR5vhExCPhP4NIe6LPbDejtBvqaiHgA2K+doYs7e4h2\ntmVEHAYclJn/0fa8ZW/rrjm3Ov4AYCFwfWau2/kOe8T7zqGDms7suyuqZs7NgxETgKuBE2vYV3ep\nZr6XAtdm5tbKG4g+zWDYSZl5/I7GIuLViBiRma9ExAjgr+2UbQCmtVofBSwBpgJHRMR6mn8u+0TE\nksycRi/rxjlvNx9Yk5nX1aDd7rIBOKDV+ijg5R3UbKiE3WBgYyf33RVVM2ciYhTwc+ArmflC97db\ntWrmexQwKyK+DwwB3ouItzPzhu5vuxv09kWO3ekB/G/KC7Hfb6dmGPAizRdfh1aWh7WpGUPfufhc\n1Zxpvp5yL9Cvt+fSwTwH0Hz+eCz//8LkhDY151JemLy7sjyB8uLzOvrGxedq5jykUv/53p5HT8y3\nTc0l9PGLz73ewO70oPnc6oPAmsrz9n/86oGbW9V9leYLkGuBs9s5Tl8Khi7Pmeb/kSXwLLCi8vh6\nb8/pfeb6GeCPNN+5cnFl22XA5yrLA2m+I2Ut8F/Av7Xa9+LKfs+zi955Vcs5A/8L+J9WP9cVwD69\nPZ/u/Bm3OkafDwY/+SxJKnhXkiSpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgr/D8L1\n33e2p9waAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22b08182908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "for i in [1, 140, 400, 750]:\n",
    "    plt.text(U[i,0], U[i,1], dictionary[i])\n",
    "    plt.axis(\"tight\")\n",
    "\n",
    "    #how to plot this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sen2vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-aa4fc1971015>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msen2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthresholded_wordlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sen2vec' is not defined"
     ]
    }
   ],
   "source": [
    "sen2vec(thresholded_wordlist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sen2vec(sen):\n",
    "    l=len(sen)\n",
    "    #X_train=np.zeros()\n",
    "#     print(l)\n",
    "    idx=dictionary.doc2idx(sen)\n",
    "#     print(idx)\n",
    "    vec=U_new[idx]\n",
    "#     print(vec)\n",
    "    total_sum=sum(vec) #feature wise sum (column)\n",
    "#     avg=total_sum/l\n",
    "#     print(total_sum)\n",
    "    \n",
    "    return total_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "# X_train=np.zeros((len(thresholded_wordlist),len(U_new)))\n",
    "X_train = []\n",
    "for sen in thresholded_wordlist:\n",
    "    X_train.append(sen2vec(sen))\n",
    "    #print(X_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=np.asarray(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 50)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression()\n",
    "lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.837"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testDataGen():\n",
    "    wordlist_test = []\n",
    "    for i in tqdm(range(df_test.shape[0])):\n",
    "        wordlist_test.append(process_text(df_test['review'].iloc[i]))        \n",
    "    return wordlist_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:14<00:00, 70.96it/s]\n"
     ]
    }
   ],
   "source": [
    "wordlist_test=testDataGen()\n",
    "X_test = []\n",
    "for sen in wordlist_test:\n",
    "    X_test.append(sen2vec(sen))\n",
    "X_test=np.asarray(X_test)\n",
    "Y_test=np.concatenate((np.ones(500), np.zeros(500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 50)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression()\n",
    "lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.726"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[430,  70],\n",
       "       [204, 296]], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(Y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'U_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-e743cee1a842>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mU_new\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'U_new' is not defined"
     ]
    }
   ],
   "source": [
    "U_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.3 |Anaconda custom (64-bit)| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.0'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
