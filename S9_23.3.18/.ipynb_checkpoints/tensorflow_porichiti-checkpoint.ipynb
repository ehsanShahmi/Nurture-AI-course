{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using tensorflow as this creates a graphical structure....as a result, all the backpropagation cud be done easily by itself...\n",
    "#### all the derivateives and updates of weights are also done by tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow doesnt work with numpy....need to do all things with tensorflow-thngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_pixels = 784\n",
    "num_classes = 10\n",
    "X = tf.placeholder(tf.float32, [None, num_pixels])\n",
    "y_ = tf.placeholder(tf.float32, [None, num_classes])\n",
    "# X and Y is  a fixed constant thing.....thats y, we used placeholder\n",
    "# None dilum then we can fit any batch size data....that is any number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#W = tf.Variable(tf.zeros([num_pixels, 3]))   # This was using just a single layer\n",
    "# W has to be changed....so used variable\n",
    "\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([num_pixels, 200]))  # now using three layers\n",
    "W2 = tf.Variable(tf.random_normal([200, 150]))\n",
    "W3 = tf.Variable(tf.random_normal([150, num_classes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A1 = tf.nn.sigmoid(tf.matmul(X, W1))             # using the created 3 layers\n",
    "A2 = tf.nn.sigmoid(tf.matmul(A1, W2))\n",
    "y = tf.nn.softmax(tf.matmul(A2, W3))        # this y is the final predicted y...from the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(X, W)) # this is the predicted y, found by our classifier \n",
    "                                    # if using just a single layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### y_ ===>>> y_real\n",
    "#### y ===>>> y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.025850929940457"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding cost using numpy:\n",
    "\n",
    "import numpy as np\n",
    "y_real = np.ones((10,))\n",
    "y_pred = np.ones((10,)) * 0.1\n",
    "- np.mean(np.sum(y_real * np.log(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BUT , we will be using tensorflow:\n",
    "cost = tf.reduce_mean(tf.reduce_sum(y_ * tf.log(y), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this to just minimize the cost by this learning rate\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializations are done....now tensorflow is run using sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Now loading the dataset first\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss -13.975600242614746 at iteration 1\n",
      "Loss -14.417806625366211 at iteration 2\n",
      "Loss -16.847919464111328 at iteration 3\n",
      "Loss -15.733990669250488 at iteration 4\n",
      "Loss -18.57220458984375 at iteration 5\n",
      "Loss -18.58596420288086 at iteration 6\n",
      "Loss -19.980239868164062 at iteration 7\n",
      "Loss -22.428913116455078 at iteration 8\n",
      "Loss -20.478046417236328 at iteration 9\n",
      "Loss -23.356645584106445 at iteration 10\n",
      "Loss -21.122968673706055 at iteration 11\n",
      "Loss -24.238691329956055 at iteration 12\n",
      "Loss -25.976957321166992 at iteration 13\n",
      "Loss -26.284055709838867 at iteration 14\n",
      "Loss -25.232383728027344 at iteration 15\n",
      "Loss -27.614154815673828 at iteration 16\n",
      "Loss -26.162843704223633 at iteration 17\n",
      "Loss -27.285459518432617 at iteration 18\n",
      "Loss -27.611478805541992 at iteration 19\n",
      "Loss -30.862363815307617 at iteration 20\n",
      "Loss -30.460063934326172 at iteration 21\n",
      "Loss -32.491172790527344 at iteration 22\n",
      "Loss -34.601402282714844 at iteration 23\n",
      "Loss -35.48101043701172 at iteration 24\n",
      "Loss -33.5396728515625 at iteration 25\n",
      "Loss -33.07726287841797 at iteration 26\n",
      "Loss -33.23814392089844 at iteration 27\n",
      "Loss -40.16975402832031 at iteration 28\n",
      "Loss -36.10362243652344 at iteration 29\n",
      "Loss -35.89052963256836 at iteration 30\n",
      "Loss -38.1945686340332 at iteration 31\n",
      "Loss -41.05247116088867 at iteration 32\n",
      "Loss -44.31185531616211 at iteration 33\n",
      "Loss -43.723690032958984 at iteration 34\n",
      "Loss -45.457374572753906 at iteration 35\n",
      "Loss -41.3561897277832 at iteration 36\n",
      "Loss -49.44302749633789 at iteration 37\n",
      "Loss -48.15876388549805 at iteration 38\n",
      "Loss -43.43611145019531 at iteration 39\n",
      "Loss -49.391353607177734 at iteration 40\n",
      "Loss -44.94056701660156 at iteration 41\n",
      "Loss -47.90659713745117 at iteration 42\n",
      "Loss -48.57317352294922 at iteration 43\n",
      "Loss -51.730533599853516 at iteration 44\n",
      "Loss -53.722862243652344 at iteration 45\n",
      "Loss -50.67316436767578 at iteration 46\n",
      "Loss -50.67061996459961 at iteration 47\n",
      "Loss -50.765419006347656 at iteration 48\n",
      "Loss -53.702842712402344 at iteration 49\n",
      "Loss -54.329986572265625 at iteration 50\n",
      "Loss -57.0517578125 at iteration 51\n",
      "Loss -55.80034255981445 at iteration 52\n",
      "Loss -61.319618225097656 at iteration 53\n",
      "Loss -56.44314956665039 at iteration 54\n",
      "Loss -62.04121398925781 at iteration 55\n",
      "Loss -57.938114166259766 at iteration 56\n",
      "Loss -60.74519348144531 at iteration 57\n",
      "Loss -61.67076110839844 at iteration 58\n",
      "Loss -64.02287292480469 at iteration 59\n",
      "Loss -59.02922058105469 at iteration 60\n",
      "Loss -60.66913986206055 at iteration 61\n",
      "Loss -inf at iteration 62\n",
      "Loss nan at iteration 63\n",
      "Loss nan at iteration 64\n",
      "Loss nan at iteration 65\n",
      "Loss nan at iteration 66\n",
      "Loss nan at iteration 67\n",
      "Loss nan at iteration 68\n",
      "Loss nan at iteration 69\n",
      "Loss nan at iteration 70\n",
      "Loss nan at iteration 71\n",
      "Loss nan at iteration 72\n",
      "Loss nan at iteration 73\n",
      "Loss nan at iteration 74\n",
      "Loss nan at iteration 75\n",
      "Loss nan at iteration 76\n",
      "Loss nan at iteration 77\n",
      "Loss nan at iteration 78\n",
      "Loss nan at iteration 79\n",
      "Loss nan at iteration 80\n",
      "Loss nan at iteration 81\n",
      "Loss nan at iteration 82\n",
      "Loss nan at iteration 83\n",
      "Loss nan at iteration 84\n",
      "Loss nan at iteration 85\n",
      "Loss nan at iteration 86\n",
      "Loss nan at iteration 87\n",
      "Loss nan at iteration 88\n",
      "Loss nan at iteration 89\n",
      "Loss nan at iteration 90\n",
      "Loss nan at iteration 91\n",
      "Loss nan at iteration 92\n",
      "Loss nan at iteration 93\n",
      "Loss nan at iteration 94\n",
      "Loss nan at iteration 95\n",
      "Loss nan at iteration 96\n",
      "Loss nan at iteration 97\n",
      "Loss nan at iteration 98\n",
      "Loss nan at iteration 99\n",
      "Loss nan at iteration 100\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    \n",
    "    for i in range(100):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "        _, loss = sess.run([train_step,cost], feed_dict = {X: batch_xs, y_: batch_ys}) \n",
    "        # Here, all comptutions are dependant with each other. \n",
    "        # The LAST computation is the train_step thats y, we gave  just trains_step and so all ops from first is to b run\n",
    "        # We'd have to give separate run() for each computations, if they were independant of each other\n",
    "        # print (\"Loss {} at iteration {}\".format(loss, i+1))\n",
    "        \n",
    "        # Now we need to predict how much the right labels were found from the true labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
